<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Machine Learning Model as a service &#8211; Blog of software writer Chee Yeo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This article aims to explain and show how to deploy a pretrained model as a service running on AWS lambda">
    <meta name="author" content="Chee Yeo">
    <meta name="keywords" content="machine-learning, mxnet, terraform, devops, mlops">
    <link rel="canonical" href="https://www.cheeyeo.dev/machine-learning/mxnet/terraform/devops/mlops/2021/03/21/machine-learning-model-as-a-service/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Blog of software writer Chee Yeo" href="/feed.xml" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202211122129" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_UK">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Machine Learning Model as a service">
    <meta property="og:description" content="Chee Yeo is a software developer with interests in machine learning and cloud computing.">
    <meta property="og:url" content="https://www.cheeyeo.dev/machine-learning/mxnet/terraform/devops/mlops/2021/03/21/machine-learning-model-as-a-service/">
    <meta property="og:site_name" content="Blog of software writer Chee Yeo">
</head>

<body class="">
  <div class="color-line"></div>
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      
        <nav class="site-nav">
          <a href="/">Home</a>
          <a href="https://tilrnt.github.io/" target="_blank">TILRNT</a>
          <a href="/about">About</a>
          <a href="/contact">Contact</a>
        </nav>
      
      <div class="clearfix"></div>
    </div>
  </div>
</header>

    <header class="blog-header">
  <h1 class="blog-title">Machine Learning Model as a service</h1>

  
  <div class="meta_info">
    
    <div class="author-date-wrap">
      <div class="author">
        <a href="/about">Chee Yeo</a>
      </div>
    </div>
    
    <span class="post-date">March 21, 2021</span>
    
    <ul class="article-tag">
      
      <li>
        <a href="/categories/machine-learning">machine-learning</a>
      </li>
      
      <li>
        <a href="/categories/mxnet">mxnet</a>
      </li>
      
      <li>
        <a href="/categories/terraform">terraform</a>
      </li>
      
      <li>
        <a href="/categories/devops">devops</a>
      </li>
      
      <li>
        <a href="/categories/mlops">mlops</a>
      </li>
      
    </ul>
    
  </div>
  
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<article class="post-content top-border">
  
<p>In a <a href="https://medium.com/apache-mxnet/streaming-inference-pipeline-deploying-mxnet-model-on-aws-lambda-7ce6bc8f4cc8" target="_blank">recent post on the MXNet blog</a>, it demonstrated an example of how to build a simple model inference pipeline.</p>

<p>Normally, a deployed model sits behind an API endpoint, accepting input requests and returning a response in the form of a prediction, be it a label for classification or real valued output for regression tasks.</p>

<p>In the case of a streaming data inference pipeline, we do not know in advance the frequency of the requests as data may arrive at any point in time.</p>

<p>A better approach to modeling the above would be to use an event driven architecture, whereby the arrival of the data stream would trigger an event and delegates the request to the model. If we deploy the model as a serverless lambda function, we could utilize the model as a service similar to how microservices work and build an inference pipeline where the prediction output could be stored or forwarded to another service for processing.</p>

<h2 id="architecture">Architecture</h2>

<p>For the purpose of this article, I re-created the architecture on AWS using the following components:</p>

<ul>
  <li>
    <p>3 S3 buckets( one for iput; one for storing the model resources; one for storing the output )</p>
  </li>
  <li>
    <p>Model application code deployed as a lambda service</p>
  </li>
</ul>

<p>It is now possible to run docker containers as lambda functions by using the AWS RIC library. The model code is built and packaged as a docker image which is published onto ECR. The image is specified as a source during lambda creation. The screenshot below shows this process:</p>

<p><img src="/assets/img/lambda/figure1.png" alt="MXNet lambda function" /></p>

<p>Since we are using a pretrained resnet model, we require the model’s parameters to be loaded during inference. The model’s weights and labels are stored in a resource bucket and loaded when the lambda function runs. This allows us to also enable versioning in the S3 resource bucket to load specific model versions on request.</p>

<h2 id="automation">Automation</h2>

<p>I automated the required resources using Terraform scripts.</p>

<p>The scripts provisioned 3 S3 buckets and handled the lambda creation process. It also setups the required IAM roles for the right permissions to communicate between S3 and Lambda.</p>

<h2 id="inference">Inference</h2>

<p>An event notification is setup between the input s3 bucket to the lambda function.</p>

<p><img src="/assets/img/lambda/figure2.png" alt="S3 Bucket notification event" /></p>

<p>It emits object created events which are processed by the lambda function handler, which passes the object filename as an input to the pretrained ResNet model. The model makes an inference and stores the prediction into a text file in the output bucket.</p>

<p><img src="/assets/img/lambda/figure3.png" alt="Cloudwatch logs event of model inference" /></p>

<p><img src="/assets/img/lambda/figure4.png" alt="Output text files of predictions" /></p>

<p>The output target can be reconfigured to be a database or another lambda function as part of a processing pipeline.</p>

<p>To reduce inference time, a recommended approach is to keep the lambda function in a “warm” state. This can be done by setting up a cloudwatch event that pings the lambda function every 15 seconds for instance.</p>

<p>Using the lambda defaults, I noticed that the model has the inclination to timeout within 3 seconds due to the loading of weights. To compensate for the possible timeouts due to loading the model, we can increase the timeout to <code class="language-plaintext highlighter-rouge">30 seconds</code> and increase the memory to <code class="language-plaintext highlighter-rouge">1024MB</code> for better performance. Further tests based on response times and workloads are covered in the original article.</p>

<p>In a further post, I aim to explore the same approach but using the latest version of MXNet built with oneDNN with Operator Fusion and Quantization built in.</p>

<p>The project source code can be viewed at the <a href="https://github.com/cheeyeo/mxnet_serverless_inference">project github repo</a></p>

<p>Happy Hacking!</p>

</article>





      </div>
    </div>
  </div>

  <footer class="footer">
  <div class="p2 wrap">
    
      <div class="social-icons">
  <div class="">
    
      <a class="fa fa-github fa-lg" href="https://github.com/cheeyeo" target="_blank"></a>
    
    <a class="fa fa-rss fa-lg" href="https://www.cheeyeo.dev/feed.xml" target="_blank"></a>
    
    
    
      <a class="fa fa-envelope fa-lg" href="mailto:f/mnqwaypk"></a>
    
    
      <a class="fa fa-linkedin fa-lg" href="https://www.linkedin.com/in/cheeyeo" target="_blank"></a>
    
  </div>
</div>

    

    <div class="measure mt1 center">
      <strong>© 2022 Chee Yeo</strong><br/>
      <small>
        Built using the Pixll theme available on <a href="https://github.com/johno/pixyll" target="_blank">Github</a>.
      </small>
    </div>
  </div>
</footer>



</body>
</html>
