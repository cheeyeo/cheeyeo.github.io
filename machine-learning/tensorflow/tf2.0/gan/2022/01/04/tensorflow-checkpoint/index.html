<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Saving Model checkpoint in Tensorflow 2.0 using tf.train.Checkpoint &#8211; Blog of software writer Chee Yeo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Saving GAN model checkpoints during training in TF 2.0 using tf.train.Checkpoint">
    <meta name="author" content="Chee Yeo">
    <meta name="keywords" content="machine-learning, tensorflow, tf2.0, gan">
    <link rel="canonical" href="https://www.cheeyeo.dev/machine-learning/tensorflow/tf2.0/gan/2022/01/04/tensorflow-checkpoint/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Blog of software writer Chee Yeo" href="/feed.xml" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202302141635" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_UK">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Saving Model checkpoint in Tensorflow 2.0 using tf.train.Checkpoint">
    <meta property="og:description" content="Chee Yeo is a software developer with interests in machine learning and cloud computing.">
    <meta property="og:url" content="https://www.cheeyeo.dev/machine-learning/tensorflow/tf2.0/gan/2022/01/04/tensorflow-checkpoint/">
    <meta property="og:site_name" content="Blog of software writer Chee Yeo">
</head>

<body class="">
  <div class="color-line"></div>
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      
        <nav class="site-nav">
          <a href="/">Home</a>
          <a href="https://tilrnt.github.io/" target="_blank">TILRNT</a>
          <a href="/about">About</a>
          <a href="/contact">Contact</a>
        </nav>
      
      <div class="clearfix"></div>
    </div>
  </div>
</header>

    <header class="blog-header">
  <h1 class="blog-title">Saving Model checkpoint in Tensorflow 2.0 using tf.train.Checkpoint</h1>

  
  <div class="meta_info">
    
    <div class="author-date-wrap">
      <div class="author">
        <a href="/about">Chee Yeo</a>
      </div>
    </div>
    
    <span class="post-date">January 4, 2022</span>
    
    <ul class="article-tag">
      
      <li>
        <a href="/categories/machine-learning">machine-learning</a>
      </li>
      
      <li>
        <a href="/categories/tensorflow">tensorflow</a>
      </li>
      
      <li>
        <a href="/categories/tf2.0">tf2.0</a>
      </li>
      
      <li>
        <a href="/categories/gan">gan</a>
      </li>
      
    </ul>
    
  </div>
  
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<article class="post-content top-border">
  
<p>Whilst exploring how to build and train a GAN model in tensorflow I came upon an interesting issue on how to save the model’s checkpoints during training.</p>

<p>One can usually save a model’s checkpoint either using the built-in <strong>ModelCheckpoint</strong> callback or by using a custom callback class subclassing from <strong>Callback</strong>. The issue is with a GAN, we have two models being trained concurrently - the critic / discriminator and the generator.</p>

<p>Normally, we tend to wrap both models inside another class object or we can create a custom model class subclassed from <strong>tf.keras.models.Model</strong> and override the <strong>train_step</strong> function.</p>

<p>In both cases, we can’t define a callback on the resultant model using <strong>fit</strong> as it is only a logical wrapper around the critic/generator models.</p>

<p>If we attempt to define the checkpoint callback on the logical wrapper model we will get the following error:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="p">...</span>

<span class="nb">ValueError</span><span class="p">:</span> <span class="n">Model</span> <span class="o">&lt;</span><span class="n">models</span><span class="p">.</span><span class="n">dcgan</span><span class="p">.</span><span class="n">DCGAN</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f84463f9d10</span><span class="o">&gt;</span> <span class="n">cannot</span> <span class="n">be</span> <span class="n">saved</span> <span class="n">because</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">shapes</span> <span class="n">have</span> <span class="ow">not</span> <span class="n">been</span> <span class="nb">set</span><span class="p">.</span> <span class="n">Usually</span><span class="p">,</span> <span class="nb">input</span> <span class="n">shapes</span> <span class="n">are</span> <span class="n">automatically</span> <span class="n">determined</span> <span class="k">from</span> <span class="n">calling</span> <span class="sb">`.fit()`</span> <span class="ow">or</span> <span class="sb">`.predict()`</span><span class="p">.</span> <span class="n">To</span> <span class="n">manually</span> <span class="nb">set</span> <span class="n">the</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">call</span> <span class="sb">`model.build(input_shape)`</span><span class="p">.</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>As per the <a href="https://www.tensorflow.org/tutorials/generative/dcgan">TF 2.0 Guide on training GAN</a>, it uses an object of <strong>tf.train.Checkpoint</strong> to save the checkpoints of the optimizers, generator and critic models during training.</p>

<p>Using the above approach, we can create a custom callback that would allow us to save the current checkpoint per epoch:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">EpochCheckpoint</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">,</span> <span class="n">every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">start_at</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ckpt_obj</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EpochCheckpoint</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">output_dir</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">every</span> <span class="o">=</span> <span class="n">every</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">int_epoch</span> <span class="o">=</span> <span class="n">start_at</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ckpt_obj</span>

    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">int_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s">"ckpt"</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_prefix</span><span class="o">=</span><span class="n">checkpoint_prefix</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">int_epoch</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Firstly, we subclass <strong>Callback</strong> class and initialize some instance variables:</p>

<ul>
  <li>
    <p><strong>checkpoint_dir</strong> where the checkpoint is to be saved</p>
  </li>
  <li>
    <p><strong>every</strong>, frequency at which we save per epoch</p>
  </li>
  <li>
    <p><strong>int_epoch</strong>, when to start saving the checkpoint, Defaults to epoch 0</p>
  </li>
  <li>
    <p><strong>checkpoint</strong>, the <strong>tf.train.Checkpoint</strong> object which gets passed from the training script containing the optimizers and models to save</p>
  </li>
</ul>

<p>We override the <strong>on_epoch_end</strong> function to save the checkpoints at the end of each epoch. Within the function call, we initialize the prefix to save the checkpoint and calls the <strong>save</strong> method of the checkpoint object. Then we increment the internal counter to track the current epoch number.</p>

<p>Within the main training script, we need to initialize the above callback and define the objects we want the checkpoint to store. If training is interrupted, we need a way to resume training from the last saved checkpoint. This can be accomplished by calling <strong>tf.train.latest_checkpoint</strong>, passing in the checkpoint directory. If any checkpoints exist, it will return the filepath else it returns None.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="code"><pre><span class="n">ckpt_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">"output"</span><span class="p">,</span> <span class="s">"checkpoints"</span><span class="p">)</span>

<span class="c1"># when to start checkpoint; will be 0 when first training
</span><span class="n">start_at</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Define the objects we want TF to track
</span><span class="n">ckpt_obj</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Checkpoint</span><span class="p">(</span>
        <span class="n">d_opt</span><span class="o">=</span><span class="n">d_opt</span><span class="p">,</span>
        <span class="n">g_opt</span><span class="o">=</span><span class="n">g_opt</span><span class="p">,</span>
        <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
        <span class="n">discriminator</span><span class="o">=</span><span class="n">discriminator</span>
<span class="p">)</span>

<span class="n">latest_ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">)</span>

<span class="k">if</span> <span class="n">latest_ckpt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"[INFO] Resuming from ckpt: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">latest_ckpt</span><span class="p">))</span>
    <span class="n">ckpt_obj</span><span class="p">.</span><span class="n">restore</span><span class="p">(</span><span class="n">latest_ckpt</span><span class="p">).</span><span class="n">expect_partial</span><span class="p">()</span>
    <span class="n">latest_ckpt_idx</span> <span class="o">=</span> <span class="n">latest_ckpt</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">sep</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">split</span><span class="p">(</span><span class="s">"-"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">start_at</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">latest_ckpt_idx</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Resuming ckpt at </span><span class="si">{</span><span class="n">start_at</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">EpochCheckpoint</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">,</span> <span class="n">every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">start_at</span><span class="o">=</span><span class="n">start_at</span><span class="p">,</span> <span class="n">ckpt_obj</span><span class="o">=</span><span class="n">ckpt_obj</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_imgs</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ckpt_callback</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>From above, we define the checkpoint directory. Next we create a <strong>tf.train.Checkpoint</strong> object. This allows us to define the objects we wish to track using a dictionary. In this case, we define the two optimizers and the generator and critic models.</p>

<p>Next we check if we are resuming training from previous checkpoint by calling <strong>tf.train.latest_checkpoint</strong>.</p>

<p>If this is the first time we are running the training script, there will be no checkpoints and this will return None. The script will continue to call fit and start training from scratch.</p>

<p>If there are any checkpoints found, it will call <strong>restore</strong> on the checkpoint object and attempt to extract the epoch number from its filepath. This sets the <strong>start_at</strong> variable which gets passed into the callback object to resume training from that specific checkpoint found.</p>

<p>Full working example can be found on <a href="https://www.tensorflow.org/tutorials/generative/dcgan">TF 2.0 Guide on training GAN</a> and the official <a href="https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint">tf.train.Checkpoint API</a> has implementation examples.</p>

<p>Happy Hacking !!!</p>

</article>





      </div>
    </div>
  </div>

  <footer class="footer">
  <div class="p2 wrap">
    
      <div class="social-icons">
  <div class="">
    
      <a class="fa fa-github fa-lg" href="https://github.com/cheeyeo" target="_blank"></a>
    
    <a class="fa fa-rss fa-lg" href="https://www.cheeyeo.dev/feed.xml" target="_blank"></a>
    
    
    
      <a class="fa fa-envelope fa-lg" href="mailto:f/mnqwaypk"></a>
    
    
      <a class="fa fa-linkedin fa-lg" href="https://www.linkedin.com/in/cheeyeo" target="_blank"></a>
    
  </div>
</div>

    

    <div class="measure mt1 center">
      <strong>© 2023 Chee Yeo</strong><br/>
      <small>
        Built using the Pixll theme available on <a href="https://github.com/johno/pixyll" target="_blank">Github</a>.
      </small>
    </div>
  </div>
</footer>



</body>
</html>
