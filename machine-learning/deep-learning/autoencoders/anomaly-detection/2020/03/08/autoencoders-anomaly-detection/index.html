<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Anomaly detection with Autoencoders &#8211; Blog of software writer Chee Yeo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="How to perform anomaly detection using Autoencoders">
    <meta name="author" content="Chee Yeo">
    <meta name="keywords" content="machine-learning, deep-learning, autoencoders, anomaly-detection">
    <link rel="canonical" href="https://www.cheeyeo.dev/machine-learning/deep-learning/autoencoders/anomaly-detection/2020/03/08/autoencoders-anomaly-detection/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Blog of software writer Chee Yeo" href="/feed.xml" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202211122129" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_UK">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Anomaly detection with Autoencoders">
    <meta property="og:description" content="Chee Yeo is a software developer with interests in machine learning and cloud computing.">
    <meta property="og:url" content="https://www.cheeyeo.dev/machine-learning/deep-learning/autoencoders/anomaly-detection/2020/03/08/autoencoders-anomaly-detection/">
    <meta property="og:site_name" content="Blog of software writer Chee Yeo">
</head>

<body class="">
  <div class="color-line"></div>
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      
        <nav class="site-nav">
          <a href="/">Home</a>
          <a href="https://tilrnt.github.io/" target="_blank">TILRNT</a>
          <a href="/about">About</a>
          <a href="/contact">Contact</a>
        </nav>
      
      <div class="clearfix"></div>
    </div>
  </div>
</header>

    <header class="blog-header">
  <h1 class="blog-title">Anomaly detection with Autoencoders</h1>

  
  <div class="meta_info">
    
    <div class="author-date-wrap">
      <div class="author">
        <a href="/about">Chee Yeo</a>
      </div>
    </div>
    
    <span class="post-date">March 8, 2020</span>
    
    <ul class="article-tag">
      
      <li>
        <a href="/categories/machine-learning">machine-learning</a>
      </li>
      
      <li>
        <a href="/categories/deep-learning">deep-learning</a>
      </li>
      
      <li>
        <a href="/categories/autoencoders">autoencoders</a>
      </li>
      
      <li>
        <a href="/categories/anomaly-detection">anomaly-detection</a>
      </li>
      
    </ul>
    
  </div>
  
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<article class="post-content top-border">
  <p>In my recent studies on computer vision and machine learning, I came across the concepts of autoencoders. Essentially, Autoencoders are neural networks which are trained using unsupervised learning.</p>

<p>The aim of training an autoencoder is to learn a compressed representation of a given input. Internally, the network is able to learn the salient features of the data through successive layers and compresses it into a latent space representation. In some ways, this is similar to other algorithms such as PCA.</p>

<p>The learned compressed latent space representation can be used in areas such as dimensionality reduction; denoising; and anomaly detection, which is the focus of this post.</p>

<p>An autoencoder consists of 2 main components: encoder, decoder. The encoder is responsible for learning a latent representation of the input. The decoder tries to reconstruct the original input from the learned latent space.</p>

<p>While there are speciality algorithms for anomaly detection such as IsolationForests; DBScan; and RCF, we will attempt to construct and autoencoder using tensorflow.</p>

<p>Since we are dealing with image inputs, we can build a convolutional autoencoder. The model will comprise of convolutional layers which <code class="language-plaintext highlighter-rouge">downsamples</code> the input in the encoder model; and convolutional transpose layers to <code class="language-plaintext highlighter-rouge">upsample</code> the input in the decoder model to reconstruct the latent representation back into its original dimension.</p>

<p>An example autoencoder could be as follows:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre></td><td class="code"><pre><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
<span class="n">chan_dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">if</span> <span class="n">K</span><span class="p">.</span><span class="n">image_data_format</span><span class="p">()</span> <span class="o">==</span> <span class="s">"channels_first"</span><span class="p">:</span>
	<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
	<span class="n">chan_dim</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>

<span class="c1"># Build encoder
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">filters</span><span class="p">:</span>
	<span class="c1"># use strided convolutions to downsample
</span>	<span class="c1"># no pooling layers...
</span>	<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">chan_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">vol_size</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">int_shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">latent</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">latent</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"encoder"</span><span class="p">)</span>

<span class="c1"># Decoder which accepts output of encoder as input
</span><span class="n">latent_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">prod</span><span class="p">(</span><span class="n">vol_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))(</span><span class="n">latent_inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Reshape</span><span class="p">((</span><span class="n">vol_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vol_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">vol_size</span><span class="p">[</span><span class="mi">3</span><span class="p">]))(</span><span class="n">x</span><span class="p">)</span>

<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">filters</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
	<span class="c1"># apply CONV_TRANSPOSE =&gt; RELU =&gt; BN
</span>	<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">chan_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># apply single conv transpose layer to recover original depth of image
</span><span class="n">x</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">"sigmoid"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">latent_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"decoder"</span><span class="p">)</span>

<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s">"autoencoder"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Normally in a typical CNN model, we use <code class="language-plaintext highlighter-rouge">MaxPooling</code> to downsample the input dimensions. In this case, we are using <code class="language-plaintext highlighter-rouge">strides</code> of 2 to reduce the dimensions by half. By the fully-connected layers of the Encoder model, the input is reduced down to the specified <code class="language-plaintext highlighter-rouge">latent_dim</code> of 16.</p>

<p>For the decoder, we use a <code class="language-plaintext highlighter-rouge">convolutional transpose</code> to upsample the latent dimension back to its original size. The outputs are then passed through a <code class="language-plaintext highlighter-rouge">sigmoid</code> activation layer as we have scaled our image inputs to be in the range of <code class="language-plaintext highlighter-rouge">[0,1]</code>.</p>

<p>The two models are then used to construct the <code class="language-plaintext highlighter-rouge">autoencoder</code> model which is the model used for training and evaluation. Note that in the output of the autoencoder model, we specified the following:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>  <span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Since our aim is to train a model to reconstruct the inputs, it follows that we want our model to learn the following scoring function, given X as input:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>  <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">E</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>From my own experimentation, I found that most of the articles or books I came across use MNIST as the dataset, which is in grayscale.</p>

<p>It can be difficult to train an autoencoder in RGB images as they are of higher dimensionality with 3 colour channels. I had to make the following alterations to the training process as follows:</p>

<ul>
  <li>
    <p>Increased the number of epochs to at least 100</p>
  </li>
  <li>
    <p>Increased the latent dimensionality of model to be at least 128</p>
  </li>
  <li>
    <p>Use a high learning rate to start training and gradually reduce it using <code class="language-plaintext highlighter-rouge">weight decay</code></p>
  </li>
  <li>
    <p>Given the small number of training images, I used the <code class="language-plaintext highlighter-rouge">ImageDataGenerator</code> to augment the training set. I also used dropout and weight decay on the model to combat overfitting.</p>
  </li>
</ul>

<p>The loss function was defined to the <code class="language-plaintext highlighter-rouge">mean-squared-error(MSE)</code> between the original image and the reconstructed image. During evaluation, we compute the MSE as the reconstruction loss and the lower the loss, it means the model has learnt a useful latent representation of the inputs and is able to reconstruct it.</p>

<p>For this given example, I have trained the autoencoder model on 328 RGB images of forests, with 20% for validation. For the test set, I gathered some random images which don’t contain any forest imagery and evaluated its MSE.</p>

<p>A loss plot of the training process is shown below:
<img src="/assets/img/anomaly/loss_plot.png" alt="Training loss plot" /></p>

<p>Both the training and validation loss curves show convergence from epoch 10 onwards. However, there is still overfitting from epoch 70 onwards as the validation loss starts to rise. The final reconstruction loss is <code class="language-plaintext highlighter-rouge">0.04396</code>.</p>

<p>A sample of the reconstruction from the final autoencoder is shown below:
<img src="/assets/img/anomaly/visualize_reconstructions.png" alt="Reconstructions visualisations" /></p>

<p>The images in the left column show the original images and the right show the reconstructed images. The reconstructed image resolution is not as clear but the model is able to capture the salient features such as the green hues and tree like shapes, which are in forest imagery.</p>

<p>The <a href="https://github.com/cheeyeo/autoencoder-anomaly-detection" target="_blank">complete code repository</a> can be found here.</p>

<p>Keep hacking and stay curious!</p>


</article>





      </div>
    </div>
  </div>

  <footer class="footer">
  <div class="p2 wrap">
    
      <div class="social-icons">
  <div class="">
    
      <a class="fa fa-github fa-lg" href="https://github.com/cheeyeo" target="_blank"></a>
    
    <a class="fa fa-rss fa-lg" href="https://www.cheeyeo.dev/feed.xml" target="_blank"></a>
    
    
    
      <a class="fa fa-envelope fa-lg" href="mailto:f/mnqwaypk"></a>
    
    
      <a class="fa fa-linkedin fa-lg" href="https://www.linkedin.com/in/cheeyeo" target="_blank"></a>
    
  </div>
</div>

    

    <div class="measure mt1 center">
      <strong>© 2022 Chee Yeo</strong><br/>
      <small>
        Built using the Pixll theme available on <a href="https://github.com/johno/pixyll" target="_blank">Github</a>.
      </small>
    </div>
  </div>
</footer>



</body>
</html>
