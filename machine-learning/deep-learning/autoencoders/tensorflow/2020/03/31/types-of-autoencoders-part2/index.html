<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Types of Autoencoders Part 2 - Denoising Autoencoder &#8211; Blog of software writer Chee Yeo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Introduction to Denoising Autoencoders">
    <meta name="author" content="Chee Yeo">
    <meta name="keywords" content="machine-learning, deep-learning, autoencoders, tensorflow">
    <link rel="canonical" href="https://www.cheeyeo.dev/machine-learning/deep-learning/autoencoders/tensorflow/2020/03/31/types-of-autoencoders-part2/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Blog of software writer Chee Yeo" href="/feed.xml" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202302022040" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_UK">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Types of Autoencoders Part 2 - Denoising Autoencoder">
    <meta property="og:description" content="Chee Yeo is a software developer with interests in machine learning and cloud computing.">
    <meta property="og:url" content="https://www.cheeyeo.dev/machine-learning/deep-learning/autoencoders/tensorflow/2020/03/31/types-of-autoencoders-part2/">
    <meta property="og:site_name" content="Blog of software writer Chee Yeo">
</head>

<body class="">
  <div class="color-line"></div>
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      
        <nav class="site-nav">
          <a href="/">Home</a>
          <a href="https://tilrnt.github.io/" target="_blank">TILRNT</a>
          <a href="/about">About</a>
          <a href="/contact">Contact</a>
        </nav>
      
      <div class="clearfix"></div>
    </div>
  </div>
</header>

    <header class="blog-header">
  <h1 class="blog-title">Types of Autoencoders Part 2 - Denoising Autoencoder</h1>

  
  <div class="meta_info">
    
    <div class="author-date-wrap">
      <div class="author">
        <a href="/about">Chee Yeo</a>
      </div>
    </div>
    
    <span class="post-date">March 31, 2020</span>
    
    <ul class="article-tag">
      
      <li>
        <a href="/categories/machine-learning">machine-learning</a>
      </li>
      
      <li>
        <a href="/categories/deep-learning">deep-learning</a>
      </li>
      
      <li>
        <a href="/categories/autoencoders">autoencoders</a>
      </li>
      
      <li>
        <a href="/categories/tensorflow">tensorflow</a>
      </li>
      
    </ul>
    
  </div>
  
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<article class="post-content top-border">
  <p>In this post, I aim to introduce the concepts behind Denoising Autoencoders (DAE) and how it differs from the other types of autoencoders we have seen so far.</p>

<p>DAE are autoencoders that receives a corrupted data point and trained to predict original, uncorrupted data point as output.</p>

<p>Structurally, DAE are no different than undercomplete/overcomplete autoencoders. The main difference is in the training inputs. In undercomplete/overcomplete autoencoders, we pass in the original datapoint x and attempt to get the encoder to learn a latent representation of the dataset.</p>

<p>In the case of DAE, the original input data has been corrupted, for instance, by adding random Gaussian noise to an image. The corrupted input, with the original input, are fed as a training pair into the DAE i.e. <code class="language-plaintext highlighter-rouge">(~x, x)</code>.</p>

<p>Given D as decoder, E as encoder, ~x as corrupted datapoint, x as original datapoint, the DAE is trained to minimize the following loss function:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">L</span><span class="p">(</span><span class="n">D</span><span class="p">(</span><span class="n">E</span><span class="p">(</span><span class="o">~</span><span class="n">x</span><span class="p">)),</span> <span class="n">x</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>which could be rewritten as the negative log likelihood of the reconstruction distribution of the decoder:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span> <span class="n">D</span><span class="p">(</span><span class="n">x</span> <span class="o">|</span> <span class="n">h</span><span class="o">=</span><span class="n">E</span><span class="p">(</span><span class="o">~</span><span class="n">x</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>By minimizing this loss, it encourages the DAE to learn a vector field that estimates the score of the data distribution of x.</p>

<p><code class="language-plaintext highlighter-rouge">D(E(~x))</code> estimates the center of mass of the <code class="language-plaintext highlighter-rouge">clean points x</code> that could derive from ~x.</p>

<p>Although DAEs are used for denoising, it also learns a good internal representation of the dataset as a side effect of learning to denoise.</p>

<p>In terms of implementation, the model architecture is still the same as an undercomplete autoencoder. The only difference is that the training data now comprises of pairs of (corrupted, clean) datapoints as input. i.e. <code class="language-plaintext highlighter-rouge">(~x, x)</code></p>

<p>In my own research and experimentations, I managed to build a simple denoising autoencoder on the MNIST dataset. The image below shows a generated sample of corrupted/clean data pairs the autoencoder was able to learn from.</p>

<h5 id="denoising-autoencoder-for-mnist">Denoising autoencoder for MNIST</h5>
<p><img src="/assets/img/autoencoders/mnist_output_denoising.png" alt="MNIST Denoise" /></p>

<p>I applied the same autoencoder with parameter changes to the <a href="https://www.kaggle.com/c/denoising-dirty-documents" target="_blank">Kaggle Dirty Documents dataset</a>. The images on the left are the corrupted images with a noisy background and the images on the right represent the cleaned images. Although the autoencoder was able to remove the noise, more tuning and training is required for the autoencoder to recognise the various font types and sizes for each document.</p>

<h5 id="denoising-autoencoder-for-kaggle-dataset">Denoising autoencoder for Kaggle dataset</h5>
<p><img src="/assets/img/autoencoders/kaggle_denoise_dirty_documents.jpg" alt="Kaggle Denoise" /></p>

<p>For a working example of building a DAE, please refer to this <a href="https://blog.keras.io/building-autoencoders-in-keras.html" target="_blank">keras blog post</a> or this <a href="https://www.pyimagesearch.com/2020/02/24/denoising-autoencoders-with-keras-tensorflow-and-deep-learning/" target="_blank">denoising autoencoder example</a> for a more up to date example.</p>

<p>In this post, I aim to introduce what a Denoising Autoencoder is and how it differs from other autoencoders.</p>

<p>Happy Hacking.</p>


</article>





      </div>
    </div>
  </div>

  <footer class="footer">
  <div class="p2 wrap">
    
      <div class="social-icons">
  <div class="">
    
      <a class="fa fa-github fa-lg" href="https://github.com/cheeyeo" target="_blank"></a>
    
    <a class="fa fa-rss fa-lg" href="https://www.cheeyeo.dev/feed.xml" target="_blank"></a>
    
    
    
      <a class="fa fa-envelope fa-lg" href="mailto:f/mnqwaypk"></a>
    
    
      <a class="fa fa-linkedin fa-lg" href="https://www.linkedin.com/in/cheeyeo" target="_blank"></a>
    
  </div>
</div>

    

    <div class="measure mt1 center">
      <strong>Â© 2023 Chee Yeo</strong><br/>
      <small>
        Built using the Pixll theme available on <a href="https://github.com/johno/pixyll" target="_blank">Github</a>.
      </small>
    </div>
  </div>
</footer>



</body>
</html>
