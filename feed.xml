<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog of software writer Chee Yeo</title>
    <description>Chee Yeo is a software developer with interests in machine learning and cloud computing.</description>
    <link>https://www.cheeyeo.dev/</link>
    <atom:link href="https://www.cheeyeo.dev/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Using CloudFront Distribution with S3</title>
        <description>
&lt;p&gt;In a recent project, I started looking into how CloudFront distributions work in relation to serving static content stored in S3 buckets.&lt;/p&gt;

&lt;p&gt;In this article I aim to explain how to setup a cloudfront CDN with an existing private S3 bucket with compression.&lt;/p&gt;

&lt;p&gt;To allow access between cloudfront and the private S3 bucket we need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create an &lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#create-oac-overview&quot;&gt;Origin Access Control&lt;/a&gt; (OAC) policy to allow cloudfront to send authenticated requests to S3.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a bucket policy for the S3 origin to only allow access to it from that specific cloudfront distribution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To create the OAC in terraform, we can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws_cloudfront_origin_access_control&lt;/code&gt; resource:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;span class=&quot;k&quot;&gt;resource&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_cloudfront_origin_access_control&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;                              &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;S3Assets&quot;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;description&lt;/span&gt;                       &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;S3 Assets CDN Policy&quot;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;origin_access_control_origin_type&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;s3&quot;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;signing_behavior&lt;/span&gt;                  &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;always&quot;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;signing_protocol&lt;/span&gt;                  &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;sigv4&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For the bucket policy we can create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws_iam_policy_document&lt;/code&gt; resource:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;span class=&quot;k&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_iam_policy_document&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;allow_access_from_another_account&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;statement&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;sid&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;AllowCloudFrontServicePrincipalReadOnly&quot;&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;effect&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;principals&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Service&quot;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;identifiers&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cloudfront.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;s3:GetObject&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;resources&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;selected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;arn&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;condition&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;test&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;StringEquals&quot;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;AWS:SourceArn&quot;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;values&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_cloudfront_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;s3_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;arn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The policy only allows access to the contents of the private s3 bucket from cloudfront if its source arn of the requester matches that of the distribution.&lt;/p&gt;

&lt;p&gt;Assuming we have a data resource of the bucket we can attach to it through a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws_s3_bucket_policy&lt;/code&gt; resource:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;span class=&quot;k&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_s3_bucket&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;selected&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_bucket&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;resource&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_s3_bucket_policy&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;allow_access_from_another_account&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;selected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;id&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_iam_policy_document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;allow_access_from_another_account&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next we can create the cloudfront distribution using the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;span class=&quot;k&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_s3_bucket&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;selected&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_bucket&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Gets the AWS managed cache policy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_cloudfront_cache_policy&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;caching_optimized&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;prebuilt_policy_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;prebuilt_policy_name&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;resource&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_cloudfront_distribution&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;s3_distribution&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;origin&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;domain_name&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;selected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;bucket_regional_domain_name&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;origin_access_control_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;aws_cloudfront_origin_access_control&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;id&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;origin_id&lt;/span&gt;                &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;origin_name&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;enabled&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;enable_cdn&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;logging_config&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;include_cookies&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;bucket&lt;/span&gt;          &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_log_bucket&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.s3.amazonaws.com&quot;&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;prefix&lt;/span&gt;          &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_s3_log_prefix&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;default_cache_behavior&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;allowed_methods&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;DELETE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;GET&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;HEAD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;OPTIONS&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;PATCH&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;POST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;PUT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;cached_methods&lt;/span&gt;         &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;GET&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;HEAD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;target_origin_id&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;origin_name&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;viewer_protocol_policy&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;redirect-to-https&quot;&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;compress&lt;/span&gt;               &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;cache_policy_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;prebuilt_policy_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;aws_cloudfront_cache_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;caching_optimized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;aws_cloudfront_cache_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;compression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;id&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;restrictions&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;geo_restriction&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;restriction_type&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;whitelist&quot;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;locations&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;US&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;CA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;GB&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;DE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;tags&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;Environment&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;dev&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;viewer_certificate&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;cloudfront_default_certificate&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;origin&lt;/code&gt; block we define:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the domain name to the regional domain name of the s3 bucket&lt;/li&gt;
  &lt;li&gt;sets the origin access control (OAC) to reference the one we created earlier&lt;/li&gt;
  &lt;li&gt;provide an origin name as a reference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We declare a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logging&lt;/code&gt; config that stores the access logs of the distribution in a separately created bucket with a prefix value.&lt;/p&gt;

&lt;p&gt;Next we define the cache behaviour. Note that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;we set the compress option to true. This is required as its off by default and no compression is applied otherwise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We define a separate cache policy id as the source of this behaviour. We are using the default system defined &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Managed-CachingOptimized&lt;/code&gt; policy which enables both gzip and brotli compression but doesn’t set any query, headers, cookies.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that we also define the option of allowing the user to either use the default policy or to use a custom policy.&lt;/p&gt;

&lt;p&gt;If we are defining a custom user policy to enable compression, we could declare it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws_cloudfront_cache_policy&lt;/code&gt; resource like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Compression cache policy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;resource&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;aws_cloudfront_cache_policy&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;compression&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;prebuilt_policy_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;compression-policy&quot;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;comment&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Compresses assets from origin&quot;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;default_ttl&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;86400&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;max_ttl&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;31536000&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;min_ttl&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;parameters_in_cache_key_and_forwarded_to_origin&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;cookies_config&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;cookie_behavior&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;none&quot;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;cookies&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;items&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;headers_config&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;header_behavior&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;none&quot;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;items&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;query_strings_config&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;query_string_behavior&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;none&quot;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;query_strings&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;items&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nx&quot;&gt;enable_accept_encoding_brotli&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;enable_accept_encoding_gzip&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above is essentially the same as the built-in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Managed-CachingOptimized&lt;/code&gt;. We can attach this to a cache behaviour block through its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_policy_id&lt;/code&gt; argument.&lt;/p&gt;

&lt;p&gt;After the distrbution has been deployed, we can retrieve the distribution url and make some requests to retrieve some assets to check the headers.&lt;/p&gt;

&lt;p&gt;To test in the terminal using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; we need to set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Accept-Encoding&lt;/code&gt; header value, setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;br,gzip&lt;/code&gt; as its value.&lt;/p&gt;

&lt;p&gt;For example to test access to a javascript via cli:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -v -I -H &quot;Accept-Encoding: br,gzip,deflate&quot; ${CDN_URL}/jquery.min.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If compression is working, you should see in the response headers &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vary; Accept-Encoding&lt;/code&gt; header and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;content-encoding: br&lt;/code&gt; as we specified brotli as the first option. Replacing it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gzip&lt;/code&gt; should return &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;content-encoding: gzip&lt;/code&gt;. Subsequent requests should transition from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x-cache: Miss from cloudfront&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x-cache: Hit from cloudfront&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;There are certain conditions in which compression won’t work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If the asset has already been cached in cloudfront and compression is switched on after, cloudfront will keep returning the cached asset until it expires before returning a fresh copy with compression enabled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Only certain resource types can be compressed. Please check the list of &lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ServingCompressedFiles.html#compressed-content-cloudfront-file-types&quot;&gt;Supported Compression FileTypes&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the origin s3 bucket is configured as a website endpoint, we can’t use cloudfront as S3 doesn’t support HTTPS connections in that configuration&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cloudfront compression is set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;off&lt;/code&gt; by default. This needs to be set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;on&lt;/code&gt; before compression works.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The complete terraform code example is as follows:
&lt;script src=&quot;https://gist.github.com/cheeyeo/e932b195040994c33693ed748be7483f.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In this post, I aim to describe how to setup a CDN using Cloudfront distribution with a private s3 bucket as its origin.&lt;/p&gt;

&lt;p&gt;There are other areas which are not covered in this post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using a signed URLs or signed cookies to restrict access to cached files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using a staging distribution to test CDN.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiple cache behaviour based on different policy to improve cache hit ratio&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More information can be found at &lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-task-list.html&quot;&gt;Private Content Task List&lt;/a&gt; and &lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ConfiguringCaching.html&quot;&gt;Optimizing caching and availability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hope it helps someone.&lt;/p&gt;

&lt;p&gt;H4ppy H4ck1n6 !!!&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/terraform/aws/s3/cloudfront/devops/2023/02/02/s3-buckets-cdn/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/terraform/aws/s3/cloudfront/devops/2023/02/02/s3-buckets-cdn/</guid>
      </item>
    
      <item>
        <title>Using Terragrunt Dependencies</title>
        <description>
&lt;p&gt;Terragrunt is a powerful tool to organize and deploy your terraform modules. Rather than writing custom scripts or manually deploying your entire stack of modules by hand, terragrunt allows you to build virtual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stacks&lt;/code&gt; of your infra via the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terragrunt.hcl&lt;/code&gt; files which uses the same HCL language as used by terraform&lt;/p&gt;

&lt;p&gt;During runtime, terragrunt translates these &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terragrunt.hcl&lt;/code&gt; configs into actual terraform files in temp dir &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.terragrunt-cache&lt;/code&gt; and delegates to terraform.&lt;/p&gt;

&lt;p&gt;One of the more powerful features I find while using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terragrunt&lt;/code&gt; is the ability to define virtual stacks of your infra using &lt;a href=&quot;https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#dependency&quot;&gt;Terragrunt dependency config&lt;/a&gt; and &lt;a href=&quot;https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#dependencies&quot;&gt;Terragrunt dependencies config&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This allows you to define explicit ordering on the order you want the modules to be applied. A side effect of using this functionality is for modules to pass data downwards as outputs from one module into the next module as its inputs. Normally, this works as expected if the entire &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stack&lt;/code&gt; has been applied at the same time. If one of the modules failed during initial apply this may lead to hard to debug errors and unexpected results.&lt;/p&gt;

&lt;p&gt;Assuming we have the following structure of modules which must be run in the following sequence:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;stack
├── terragrunt.hcl
│
├── module_a
│   └── terragrunt.hcl
│
├── module_b
│   └── terragrunt.hcl
│
└── module_c
    └── terragrunt.hcl
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There is a dependency of the following order: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A -&amp;gt; B -&amp;gt; C&lt;/code&gt;. Both Module B and Module C relies on certain outputs from Module A. A common pattern is for Module B to use the outputs from Module A as inputs. These inputs are then defined as outputs in Module B, which gets passed into Module C via terragrunt.hcl&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terragrunt.hcl&lt;/code&gt; file in Module B would have a format such as:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Module B terragrunt.hcl&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;module_a&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;config_path&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;../module_a&quot;&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;mock_outputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;fake-id&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;input_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;module_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Module B variables.tf&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;input_id&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Module B outputs.tf&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Passing the input values as outputs&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;output_id&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;input_id&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above declares a dependency on Module A via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config_path&lt;/code&gt; keyword. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mock_outputs&lt;/code&gt; declare fake/mock values for module_a outputs if it has not been applied yet which gets passed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Module B&lt;/code&gt; as an input to its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input_id&lt;/code&gt; variable. This same value then gets passed as an output from Module B as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output_id&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Module C has a similar format:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Module C terragrunt.hcl&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;module_b&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;config_path&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;../module_b&quot;&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;mock_outputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;fake-id&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;input_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;module_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Module C variables.tf&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;input_id&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Assuming we run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terragrunt&lt;/code&gt; and only Module A gets deployed and persisted to state.&lt;/p&gt;

&lt;p&gt;If we re-run it again, one would expect the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependency.module_b.outputs.output_id&lt;/code&gt; to be the actual output from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependency.module_a.outputs.output_id&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Instead we get the mock value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fake-id&lt;/code&gt; as Module B has not been applied and hence has no state so its mock value is returned instead. This results in the mock value being passed downstream to Module C as an input value based on its terragrunt.hcl config, leading to difficult to diagnose errors.&lt;/p&gt;

&lt;p&gt;In other words, when we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependency&lt;/code&gt; config, &lt;strong&gt;if no state exists, the mock values are returned else it fetches and returns the real values from its state.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;From the &lt;a href=&quot;https://terragrunt.gruntwork.io/docs/features/execute-terraform-commands-on-multiple-modules-at-once/#unapplied-dependency-and-mock-outputs&quot;&gt;Terragrunt Mock Outputs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;Terragrunt will return an error indicating the dependency hasn’t been applied yet if the terraform module managed by the terragrunt config referenced in a dependency block has not been applied yet. This is because you cannot actually fetch outputs out of an unapplied Terraform module, even if there are no resources being created in the module.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One way to break this dependency chain is to refactor both modules B and C so they can both run in parallel and inherit from Module A:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-terraform&quot; data-lang=&quot;terraform&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Module B terragrunt.hcl&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;module_a&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;config_path&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;../module_a&quot;&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;mock_outputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;fake-id&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;mock_outputs_merge_strategy_with_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;shallow&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;input1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;module_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# Module C terragrunt.hcl&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;module_a&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;config_path&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;../module_a&quot;&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;mock_outputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;fake-id&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;mock_outputs_merge_strategy_with_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;shallow&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;paths&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;../module_b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;input1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;dependency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;module_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;output_id&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependencies&lt;/code&gt; block in Module C so it has to wait until after Module B is applied, maintaining the sequence.&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&quot;https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#dependencies&quot;&gt;Terragrunt dependencies config&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;The dependencies block is used to enumerate all the Terragrunt modules that need to be applied in order for this module to be able to apply. Note that this is purely for ordering the operations when using run-all commands of Terraform. This does not expose or pull in the outputs like dependency blocks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Both modules now inherit from Module A which results in either the actual / mock values being passed to it rather than ambiguous intermediate output values. We are also able to maintain the sequence between Module B and Module C.&lt;/p&gt;

&lt;p&gt;The following are what I learnt the following whilst working with terragrunt dependencies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Keep dependencies to 1 level deep and pass outputs directly between modules without going through any intermediate modules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependencies&lt;/code&gt; block instead if you don’t require the outputs from a module but need to maintain sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dependency&lt;/code&gt; block with mock outputs, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mock_outputs_merge_strategy_with_state&lt;/code&gt; to merge the actual outputs after an apply to the module’s outputs map.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hope it helps someone.&lt;/p&gt;

&lt;p&gt;H4ppy H4ck1n6 !!!&lt;/p&gt;
</description>
        <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/terraform/terragrunt/devops/2023/01/17/terragrunt-dependencies/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/terraform/terragrunt/devops/2023/01/17/terragrunt-dependencies/</guid>
      </item>
    
      <item>
        <title>Using Flink High Availability with flink operator</title>
        <description>
&lt;p&gt;Flink supports high-availability mode for both standalone and native Kubernetes via the Flink operator. This article aims to explain the purpose of HA and how to configure and run it in a local kind cluster.&lt;/p&gt;

&lt;p&gt;A flink cluster has only 1 JobManager running at a given point in time. This presents as single point of failure. If the jobmanager fails currently running jobs within the cluster will also fail and have to be restarted from scratch.&lt;/p&gt;

&lt;p&gt;Enabling HA allows the cluster to recover from such failures and ensures that streaming jobs especially can resume from its last known state via checkpoints.&lt;/p&gt;

&lt;p&gt;In a previous blog post, I mentioned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;savepoints&lt;/code&gt; and how we can resume a job from it. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkpoints&lt;/code&gt; is a mechanism provided via the HA service. When HA is enabled, for any streaming job, Flink will make regular backups of the job’s state via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkpoints&lt;/code&gt; which allows you to resume the job from in event of a cluster failure.&lt;/p&gt;

&lt;p&gt;The main difference between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;savepoints&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkpoints&lt;/code&gt; is that the former is triggred by the user while the other is managed entirely by Flink.&lt;/p&gt;

&lt;p&gt;The main purpose of having two complementary systems is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkpoints&lt;/code&gt; provide fast recoverable state in the event of cluster failures such as job manager or task manager pods being killed whereas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;savepoints&lt;/code&gt; allow for more portability and is intended for long-term uses such as Flink versions upgrade and changes to job properties.&lt;/p&gt;

&lt;p&gt;In HA mode, we can have more than 1 job manager pod running concurrently and only 1 of them is selected as the Leader via the leader election service.&lt;/p&gt;

&lt;p&gt;In this post, we are using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flink-operator&lt;/code&gt; to setup our session cluster. The default setting creates a stateful set of the jobmanager and the number of replicas is not configurable at this point. However, running a replicaset means there will always be at least 1 job manager pod running so it serves this use case.&lt;/p&gt;

&lt;p&gt;To enable HA, the following conditions must be met:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Only use local storage for the high availability checkpoints. In the kind cluster config, we can mount an additional local volume and reference it in a persistent volume. The mounted volume must also have the ownership of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9999:9999&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A service account which has permissions to edit configmaps. HA stores information on the cluster state such as the current jobmanager in these configmaps.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To create the HA volume I mounted a local volume in /tmp/flink-k8s-example on localhost to /flink-k8s-example on the node in the kind cluster config:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Cluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind.x-k8s.io/v1alpha4&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;control-plane&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kubeadmConfigPatches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kind: InitConfiguration&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;nodeRegistration:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;kubeletExtraArgs:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;node-labels: &quot;ingress-ready=true&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;extraPortMappings&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;hostPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;443&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;hostPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;443&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;extraMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;hostPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/artifacts&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containerPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/artifacts&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;hostPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/flink-k8s-example&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containerPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/flink-k8s-example&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next we create the PV, and PVC for the mounted volume:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PersistentVolume&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-pv&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;local&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;storageClassName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;manual&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;capacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hostPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/flink-k8s-example/&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-shared-pvc&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;storageClassName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;manual&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumeName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-pv&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The PV mounts the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/flink-k8s-example&lt;/code&gt; path on the node to create a volume.&lt;/p&gt;

&lt;p&gt;The service account can be created from the default cluster service account:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl create serviceaccount flink-service-account

kubectl create clusterrolebinding flink-role-binding-flink &lt;span class=&quot;nt&quot;&gt;--clusterrole&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;edit &lt;span class=&quot;nt&quot;&gt;--serviceaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default:flink-service-account
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We reference the service account in the flink cluster config:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flinkoperator.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;FlinkCluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;beam-flink-cluster&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;flinkVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1.15.2&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apache/flink:1.15.2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;flink-service-account&quot;&lt;/span&gt;
  
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The flink config needs to be updated to include the HA config:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;taskmanager.memory.process.size&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2g&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;taskmanager.data.port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6121&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;taskmanager.numberOfTaskSlots&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;parallelism.default&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&quot;&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;state.backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filesystem&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;state.backend.incremental&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;state.checkpoints.dir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;file:///flink-shared/checkpoints&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;state.savepoints.dir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;file:///flink-shared/savepoints&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;classloader.resolve-order&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;parent-first&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;execution.checkpointing.interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;60&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Kubernetes config&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kubernetes.cluster-id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;beam-flink-cluster&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kubernetes.taskmanager.service-account&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;flink-service-account&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Below for HA config&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;high-availability&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;high-availability.jobmanager.port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;50010&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;high-availability.storageDir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;file:///flink-shared/ha&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;high-availability.cluster-id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;beam-flink-cluster&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;restart-strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;fixed-delay&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;restart-strategy.fixed-delay.attempts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;10&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We are using the built in HA services factory class for high-availability. We mount the volume created previously as state storage and we set the cluster id to be the cluster name, which is required.&lt;/p&gt;

&lt;p&gt;Next we define the restart strategy and a timeout. As a precaution, I also pined the jobmanager port to 50010 as the configuration docs states that this port can be a random value when a new jobmanager is created.&lt;/p&gt;

&lt;p&gt;We define the state checkpoint interval to be 60. Note that this value must be greater than 0 for checkpointing to work. We define the cluster id, which is set to the name of the flink cluster. We also add the custom service account to the taskmanager.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.backend&lt;/code&gt; is set to filesystem. We also enable incremental checkpoint via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.backend.incremental&lt;/code&gt; which only stores diffs of checkpoints rather than entire checkpoints. Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.checkpoints.dir&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.savepoints.dir&lt;/code&gt; can also be set to remote storage locations such as s3, but the main &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;high-availability.storageDir&lt;/code&gt; has to be set to a volume.&lt;/p&gt;

&lt;p&gt;Assuming the setup is right, when we start the flink session cluster, we should see the following in the jobmanager logs:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;2022-12-11 15:29:32,308 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Create KubernetesLeaderElector beam-flink-cluster-cluster-config-map with lock identity 3e81a3e8-b718-4c9e-96ad-cd8f0eacd48e.

2022-12-11 15:29:32,309 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting to watch &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;default/beam-flink-cluster-cluster-config-map, watching &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:db45acf1-52b3-4239-91a6-7232c1c56bba
...

2022-12-11 15:29:32,378 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - New leader elected 3e81a3e8-b718-4c9e-96ad-cd8f0eacd48e &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;beam-flink-cluster-cluster-config-map.

...

2022-12-11 15:29:32,541 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@8b670c0.

2022-12-11 15:29:32,541 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Web frontend listening at http://beam-flink-cluster-jobmanager:8081.

2022-12-11 15:29:32,541 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - http://beam-flink-cluster-jobmanager:8081 was granted leadership with &lt;span class=&quot;nv&quot;&gt;leaderSessionID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;92a23e47-73ea-4564-890f-0cb39937a15a

...

2022-12-11 15:29:32,556 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@3513d214.

2022-12-11 15:29:32,556 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting resource manager service.

2022-12-11 15:29:32,556 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@7534785a.

2022-12-11 15:29:32,557 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 15:29:32,557 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 15:29:32,572 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting to watch &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;default/beam-flink-cluster-cluster-config-map, watching &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:c389c018-0bda-437a-aea3-656aa64dc47f

2022-12-11 15:29:32,572 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting to watch &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;default/beam-flink-cluster-cluster-config-map, watching &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:d25f089c-67eb-4211-8ca2-245e55409c37

2022-12-11 15:29:32,574 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - DefaultDispatcherRunner was granted leadership with leader &lt;span class=&quot;nb&quot;&gt;id &lt;/span&gt;92a23e47-73ea-4564-890f-0cb39937a15a. Creating new DispatcherLeaderProcess.

2022-12-11 15:29:32,581 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Start SessionDispatcherLeaderProcess.

2022-12-11 15:29:32,584 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Resource manager service is granted leadership with session &lt;span class=&quot;nb&quot;&gt;id &lt;/span&gt;92a23e47-73ea-4564-890f-0cb39937a15a.

2022-12-11 15:29:32,585 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Recover all persisted job graphs that are not finished, yet.

2022-12-11 15:29:32,607 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting RPC endpoint &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_0 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 15:29:32,619 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Retrieved job ids &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; from KubernetesStateHandleStore&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
2022-12-11 15:29:32,619 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Successfully recovered 0 persisted job graphs.
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The logs show that the leader election service is activated and the current job manager http://beam-flink-cluster-jobmanager:8081 is selected to be the leader. Note that the service is created automatically via the flink-operator in this case. It then creates a dispatcher and resource manager service and assign them as leader, updating the configmaps.&lt;/p&gt;

&lt;p&gt;HA automatically tracks the current leader via configmaps. It created two configmaps: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;flink cluster name&amp;gt;-cluster-config-map&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;flink cluster name&amp;gt;-configmap&lt;/code&gt;. The first configmap contains the leader election details while the second configmap contains a copy of the flink and log4j configs used in the initial cluster setup.&lt;/p&gt;

&lt;p&gt;We can use the following failure scenarios to test if HA is actually working:&lt;/p&gt;

&lt;h4 id=&quot;kill-the-current-jobmanager-pod-process&quot;&gt;Kill the current jobmanager pod process&lt;/h4&gt;

&lt;p&gt;This will terminate the jobmanager process. The logs should show it being restarted:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;jobmanager_pod_name&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; /bin/sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;kill 1&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Output of kubectl get pods:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;beam-flink-cluster-jobmanager-0    1/1     Running   1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;55s ago&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   19m
beam-flink-cluster-taskmanager-0   2/2     Running   0             19m
beam-flink-cluster-taskmanager-1   2/2     Running   0             19m
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The jobmanager logs show the same startup information as when the cluster was first created, suggesting that a new jobmanager pod was created.&lt;/p&gt;

&lt;h4 id=&quot;kill-a-taskmanager-pod-process&quot;&gt;Kill a taskmanager pod process&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;taskmanager_pod_name&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; /bin/sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;kill 1&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Output of the jobmanager logs shows that a new taskmanager process is started and registered:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;2022-12-11 15:51:47,574 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Closing TaskExecutor connection 10.244.0.18:6122-3821cd because: The TaskExecutor is shutting down.

2022-12-11 15:52:01,144 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Registering TaskManager with ResourceID 10.244.0.18:6122-fe0c19 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;akka.tcp://flink@10.244.0.18:6122/user/rpc/taskmanager_0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; at ResourceManager
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;delete-the-jobmanager-pod&quot;&gt;Delete the jobmanager pod&lt;/h4&gt;

&lt;p&gt;Delete the main jobmanager pod using:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl delete pod &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;jobmanager_pod_name&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Output of jobmanager logs:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;2022-12-11 15:55:28,061 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.

2022-12-11 15:55:28,064 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Shutting StandaloneSessionClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..

2022-12-11 15:55:28,064 INFO  org.apache.flink.runtime.blob.BlobServer                     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopped BLOB server at 0.0.0.0:6124

2022-12-11 15:55:28,068 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Shutting down rest endpoint.

2022-12-11 15:55:28,078 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Removing cache directory /tmp/flink-web-0b1e5be0-95c2-4716-9688-44ef1748278b/flink-web-ui

2022-12-11 15:55:28,080 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping DefaultLeaderElectionService.

2022-12-11 15:55:28,090 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Shut down complete.

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Closing components.

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping DefaultLeaderRetrievalService.

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping KubernetesLeaderRetrievalDriver&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping DefaultLeaderRetrievalService.

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping KubernetesLeaderRetrievalDriver&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping DefaultLeaderElectionService.

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopped to watch &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;default/beam-flink-cluster-cluster-config-map, watching &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:dbf98f2b-6765-45be-bf62-afc1f15d84f6

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopped to watch &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;default/beam-flink-cluster-cluster-config-map, watching &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:a2cc3271-7ef2-47ee-9342-dfcbc83bbc4a

2022-12-11 15:55:28,110 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping SessionDispatcherLeaderProcess.

2022-12-11 15:55:28,110 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping resource manager service.

2022-12-11 15:55:28,110 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping dispatcher akka.tcp://flink@beam-flink-cluster-jobmanager:6123/user/rpc/dispatcher_1.

2022-12-11 15:55:28,111 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - 
Stopping DefaultLeaderElectionService.

2022-12-11 15:55:28,111 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping all currently running &lt;span class=&quot;nb&quot;&gt;jobs &lt;/span&gt;of dispatcher akka.tcp://flink@beam-flink-cluster-jobmanager:6123/user/rpc/dispatcher_1.

2022-12-11 15:55:28,112 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopped dispatcher akka.tcp://flink@beam-flink-cluster-jobmanager:6123/user/rpc/dispatcher_1.

2022-12-11 15:55:28,114 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Stopping DefaultJobGraphStore.

2022-12-11 15:55:28,117 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Closing the slot manager.

2022-12-11 15:55:28,117 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Suspending the slot manager.

2022-12-11 15:55:28,119 INFO  org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Closing DefaultMultipleComponentLeaderElectionService.

2022-12-11 15:55:28,120 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Closing org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver@5bb0c5c0.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The logs indicate that the leader election service was indeed closed when the jobmanager pod is deleted.&lt;/p&gt;

&lt;p&gt;The output of the new jobmanager pod show that the new jobmanager pod is elected as the current leader:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;2022-12-11 15:55:36,536 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Create KubernetesLeaderElector beam-flink-cluster-cluster-config-map with lock identity 05df88b5-bbd2-400f-b1e8-9d4d386b2a43.

2022-12-11 15:55:36,537 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting to watch &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;default/beam-flink-cluster-cluster-config-map, watching &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:fa91664e-7df9-4937-802b-20e3cba09bc9

...

2022-12-11 15:55:46,149 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - http://beam-flink-cluster-jobmanager:8081 was granted leadership with &lt;span class=&quot;nv&quot;&gt;leaderSessionID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;28855791-dee8-4a20-9cd1-b1d5b91be838

2022-12-11 15:55:46,149 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Resource manager service is granted leadership with session &lt;span class=&quot;nb&quot;&gt;id &lt;/span&gt;28855791-dee8-4a20-9cd1-b1d5b91be838.
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;testing-ha-with-checkpoints&quot;&gt;Testing HA with checkpoints&lt;/h3&gt;

&lt;p&gt;We can use the built-in statemachine example to simulate a long running streaming job. Then we monitor the checkpoints directory to ensure that its created.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flink/running-streamjob.png&quot; alt=&quot;Running statemachine streaming job&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The jobmanager logs should show the job running and checkpoints being saved:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;2022-12-11 16:02:21,632 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Flat Map -&amp;gt; Sink: Print to Std. Out &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1/1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;e4adc6c6d1d4b2b9e9318cfd95e65e35&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; switched from INITIALIZING to RUNNING.

2022-12-11 16:02:23,370 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Triggering checkpoint 1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;CheckpointType&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Checkpoint&apos;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;sharingFilesStrategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;FORWARD_BACKWARD&lt;span class=&quot;o&quot;&gt;})&lt;/span&gt; @ 1670774543316 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:02:23,451 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Completed checkpoint 1 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;7735 bytes, &lt;span class=&quot;nv&quot;&gt;checkpointDuration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;123 ms, &lt;span class=&quot;nv&quot;&gt;finalizationTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11 ms&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:02:25,326 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Triggering checkpoint 2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;CheckpointType&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Checkpoint&apos;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;sharingFilesStrategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;FORWARD_BACKWARD&lt;span class=&quot;o&quot;&gt;})&lt;/span&gt; @ 1670774545316 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:02:25,364 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Completed checkpoint 2 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;8320 bytes, &lt;span class=&quot;nv&quot;&gt;checkpointDuration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;23 ms, &lt;span class=&quot;nv&quot;&gt;finalizationTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;25 ms&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can try to kill the jobmanager process and it should resume the job from the last checkpoint, which was checkpoint 106 for this example:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;2022-12-11 16:06:08,109 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Recover all persisted job graphs that are not finished, yet.

2022-12-11 16:06:08,187 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Retrieved job ids &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;c506b6c290cc2d96a0e3f0eea10395c4] from KubernetesStateHandleStore&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

2022-12-11 16:06:08,188 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Trying to recover job with job &lt;span class=&quot;nb&quot;&gt;id &lt;/span&gt;c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:06:08,406 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Recovered JobGraph&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;jobId: c506b6c290cc2d96a0e3f0eea10395c4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:06:08,407 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Successfully recovered 1 persisted job graphs.

2022-12-11 16:06:09,128 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Initializing job &lt;span class=&quot;s1&quot;&gt;&apos;State machine job&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;c506b6c290cc2d96a0e3f0eea10395c4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:06:09,209 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Using restart back off &lt;span class=&quot;nb&quot;&gt;time &lt;/span&gt;strategy FixedDelayRestartBackoffTimeStrategy&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;maxNumberRestartAttempts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10, &lt;span class=&quot;nv&quot;&gt;backoffTimeMS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1000&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;State machine job &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;c506b6c290cc2d96a0e3f0eea10395c4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:06:09,230 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Recovering checkpoints from KubernetesStateHandleStore&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-c506b6c290cc2d96a0e3f0eea10395c4-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:06:09,276 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Found 1 checkpoints &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;KubernetesStateHandleStore&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-c506b6c290cc2d96a0e3f0eea10395c4-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:06:09,277 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Trying to fetch 1 checkpoints from storage.

2022-12-11 16:06:09,277 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Trying to retrieve checkpoint 106.
...

2022-12-11 16:06:09,504 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting execution of job &lt;span class=&quot;s1&quot;&gt;&apos;State machine job&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;c506b6c290cc2d96a0e3f0eea10395c4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; under job master &lt;span class=&quot;nb&quot;&gt;id &lt;/span&gt;8c15f93c28f74b20cc1b30ccb4da4cc3.

2022-12-11 16:06:09,506 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Starting scheduling with scheduling strategy &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]

2022-12-11 16:06:09,507 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Job State machine job &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;c506b6c290cc2d96a0e3f0eea10395c4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; switched from state CREATED to RUNNING.
...

2022-12-11 16:06:18,799 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Triggering checkpoint 107 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;CheckpointType&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Checkpoint&apos;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;sharingFilesStrategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;FORWARD_BACKWARD&lt;span class=&quot;o&quot;&gt;})&lt;/span&gt; @ 1670774778787 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:06:18,884 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Completed checkpoint 107 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;8338 bytes, &lt;span class=&quot;nv&quot;&gt;checkpointDuration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;72 ms, &lt;span class=&quot;nv&quot;&gt;finalizationTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;25 ms&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

2022-12-11 16:06:20,803 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Triggering checkpoint 108 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;CheckpointType&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Checkpoint&apos;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;sharingFilesStrategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;FORWARD_BACKWARD&lt;span class=&quot;o&quot;&gt;})&lt;/span&gt; @ 1670774780787 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:06:20,843 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Completed checkpoint 108 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job c506b6c290cc2d96a0e3f0eea10395c4 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;15169 bytes, &lt;span class=&quot;nv&quot;&gt;checkpointDuration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;32 ms, &lt;span class=&quot;nv&quot;&gt;finalizationTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;24 ms&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As can be seen above, the job was restored and continued to create checkpoints from its last checkpoint.&lt;/p&gt;

&lt;p&gt;When the job is stopped/cancelled manually, the HA data, including the checkpoints will also be automatically deleted:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;...

2022-12-11 16:16:34,782 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Clean up the high availability data &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job 034b3fe2d4f673aed68b38e787f8edf0.

2022-12-11 16:16:34,788 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Finished cleaning up the high availability data &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;job 034b3fe2d4f673aed68b38e787f8edf0.

2022-12-11 16:16:34,797 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; - Removed job graph 034b3fe2d4f673aed68b38e787f8edf0 from KubernetesStateHandleStore&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;configMapName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;beam-flink-cluster-cluster-config-map&apos;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This is because checkpoints, by default, are only used to resume a job from failures. However you can set the application configuration to retain the checkpoint on job cancellation.&lt;/p&gt;

&lt;p&gt;Further information can be found in the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLINK/FLIP-144:+Native+Kubernetes+HA+for+Flink#FLIP144:NativeKubernetesHAforFlink-ConfigMap&quot;&gt;Documentation on Flink HA&lt;/a&gt;. Details about Flink configuration can be found on &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/config/&quot;&gt;Flink 1.15.2 configuration&lt;/a&gt; page. More information can also be found on &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints/&quot;&gt;Flink Checkpoints&lt;/a&gt; and &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/&quot;&gt;Checkpoints vs savepoints&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;This post attempts to explain how to setup high availability for a flink cluster running locally in a kind cluster but the same can be applied in an actual deployed flink cluster. In that case, the state backend should be changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rocksdb&lt;/code&gt; as well as other tweaks to the configuration which is for another article.&lt;/p&gt;

&lt;p&gt;H4ppy H4ck1n6 !!!&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/2022/12/02/flink-high-availability/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/2022/12/02/flink-high-availability/</guid>
      </item>
    
      <item>
        <title>Storing Flink savepoint and artifacts in AWS S3</title>
        <description>
&lt;p&gt;We can run jobs in Flink in either Batch mode or Streaming mode. In streaming mode, we are able to save the state of the job in a specified interval. This allows for the job to resume from a given state in case of failure. These saved states are known as &lt;strong&gt;savepoints&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A thorough discussion on the subject is beyond the scope of this post so I refer the avid reader to the &lt;a href=&quot;https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/savepoints&quot;&gt;Flink guide on savepoints&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An important distinction is that &lt;strong&gt;savepoints&lt;/strong&gt; are different from &lt;strong&gt;checkpoints&lt;/strong&gt; in the sense that the former requires user intervention while the latter is invoked by the flink cluster through for example in HA (High Availability) mode.&lt;/p&gt;

&lt;p&gt;An important thing to note is that savepoints can only be invoked for streaming jobs; it won’t work for batch jobs.&lt;/p&gt;

&lt;p&gt;In order to test such a functionality with the Flink Operator, we use the &lt;a href=&quot;https://github.com/spotify/flink-on-k8s-operator/blob/master/docs/kafka_test_guide.md&quot;&gt;Kafka test guide&lt;/a&gt; example, which provides a streaming example using kafka pipelines.&lt;/p&gt;

&lt;p&gt;The rest of this post assumes you have a running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind&lt;/code&gt; cluster with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flink operator&lt;/code&gt; installed. If not, please refer to the previous posts on the subject.&lt;/p&gt;

&lt;p&gt;The following steps detail how to use such an image in a streaming job running in a kubernetes cluster locally via the flink operator:&lt;/p&gt;

&lt;h3 id=&quot;1-build-the-kafka-example&quot;&gt;1. Build the kafka example&lt;/h3&gt;

&lt;p&gt;We need to clone the &lt;a href=&quot;https://github.com/apache/flink-playgrounds/tree/master/docker/data-generator&quot;&gt;Flink playground example&lt;/a&gt; and make the following changes to the Dockerfile:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-docker&quot; data-lang=&quot;docker&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c&quot;&gt;###############################################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Build Operations Playground Image&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###############################################################################&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; apache/flink:1.15.2-scala_2.12-java8&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /opt/flink/bin&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Copy s3 plugins&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ../ &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; plugins/s3-fs-hadoop &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;opt/flink-s3-fs-hadoop-1.15.2.jar plugins/s3-fs-hadoop

&lt;span class=&quot;c&quot;&gt;# Copy Click Count Job&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; --from=builder /opt/flink-playground-clickcountjob/target/flink-playground-clickcountjob-*.jar /opt/ClickCountJob.jar&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To save &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;savepoints&lt;/code&gt; to remote cloud storage such as S3, we need to enable the filestorage plugins within the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apache/flink&lt;/code&gt; image. This is done by copying the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/flink/opt/flink-s3-fs-hadoop-1.15.2.jar&lt;/code&gt; into its own plugin directory of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/flink/plugins/flink-s3-fs-hadoop-1.15.2.jar&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once the custom image is built we can load it into the kind cluster&lt;/p&gt;

&lt;h3 id=&quot;2-install-kafka&quot;&gt;2. Install kafka&lt;/h3&gt;

&lt;p&gt;Next we need to install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm&lt;/code&gt; repo for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka&lt;/code&gt; cluster. The following script is adapted from &lt;a href=&quot;https://github.com/spotify/flink-on-k8s-operator/blob/master/docs/kafka_test_guide.md&quot;&gt;Kafka test guide&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl create ns kafka

helm repo add incubator https://charts.helm.sh/incubator

helm &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;my-kafka incubator/kafka &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; kafka

helm status my-kafka &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kafka
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Make the script executable and run it. Check that the namespace has the pods running.&lt;/p&gt;

&lt;h3 id=&quot;3-create-the-deployment&quot;&gt;3. Create the deployment&lt;/h3&gt;

&lt;p&gt;We need to create a generator deployment that writes data to the kafka cluster. This is adopted from the &lt;a href=&quot;https://github.com/spotify/flink-on-k8s-operator/blob/master/docs/kafka_test_guide.md&quot;&gt;Kafka test guide&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Example from the flink playground&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Deployment that writes data to kafka cluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-click-generator&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-click-generator&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-click-generator&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-click-generator&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m1l0/flink-ops-playground:1.15.2-scala_2.12&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;java&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-classpath&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/opt/ClickCountJob.jar:/opt/flink/lib/*&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;org.apache.flink.playgrounds.ops.clickcount.ClickEventGenerator&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--bootstrap.servers&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-kafka.kafka.svc.cluster.local:9092&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--topic&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;input&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that we are using our own custom image built in step 1.&lt;/p&gt;

&lt;p&gt;Create the deployment and check that the pods are running.&lt;/p&gt;

&lt;h3 id=&quot;4-create-the-consumer&quot;&gt;4. Create the consumer&lt;/h3&gt;

&lt;p&gt;The consumer of the data stream is a flink operator job that runs the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClickCount.jar&lt;/code&gt; application that consumes data from the kafka stream.&lt;/p&gt;

&lt;p&gt;The job is deployed as a standalone application cluster via the flink operator.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flinkoperator.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;FlinkCluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;clickcount&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;flinkVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1.15.2&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m1l0/flink-ops-playground:1.15.2-scala_2.12&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pullPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IfNotPresent&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;jobManager&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;ui&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8081&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;limits&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2Gi&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;200m&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;taskManager&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;limits&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2Gi&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;200m&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;envVars&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ENABLE_BUILT_IN_PLUGINS&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-s3-fs-hadoop-1.15.2.jar;&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AWS_ACCESS_KEY_ID&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-aws-secret&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;access_key_id&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AWS_SECRET_ACCESS_KEY&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-aws-secret&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;secret_access_key&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;jarFile&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/opt/ClickCountJob.jar&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;className&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;org.apache.flink.playgrounds.ops.clickcount.ClickEventCount&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--bootstrap.servers&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-kafka.kafka.svc.cluster.local:9092&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--checkpointing&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--event-time&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;parallelism&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;savepointsDir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s3a://flinkexps/savepoints&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;autoSavepointSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;flinkProperties&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;taskmanager.numberOfTaskSlots&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There are several important things to note here.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENABLE_BUILT_IN_PLUGINS&lt;/code&gt; is &lt;strong&gt;required&lt;/strong&gt; to allow the plugins to be copied over to the client else the job will fail with plugins not found error.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_SECRET_ACCESS_KEY_ID&lt;/code&gt; are fetched from a kubernetes secret and mounted as env vars within the job container.&lt;/p&gt;

&lt;p&gt;Within the job spec we need to declare the following 2 properties to allow for automatic savepoint creation via the flink operator.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;savePointsDir&lt;/code&gt; is the target location of our savepoints. Note that the file system prefix is &lt;strong&gt;s3a&lt;/strong&gt; as we are using the hadoop filesystem integration and it only works with that prefix.&lt;/p&gt;

&lt;p&gt;We also need to declare &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autoSavepointSeconds&lt;/code&gt; which is a non-negative integer which specifies how often to create a savepoint. In this example we set it to a lower/frequent interval of 10secs to test if it works.&lt;/p&gt;

&lt;p&gt;Apply the above configuration and check the status of the job submitted via:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl describe flinkcluster clickcount
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If all goes well, after 10 seconds, you should see a stream of savepoints being created&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flink/savepoints/cli.png&quot; alt=&quot;Flink Application CLI&quot; /&gt;
&lt;img src=&quot;/assets/img/flink/savepoints/console.png&quot; alt=&quot;AWS S3 console&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5-triggering-savepoints&quot;&gt;5. Triggering savepoints&lt;/h3&gt;

&lt;p&gt;You can also trigger the manual creation of savepoints as highlighted in the &lt;a href=&quot;https://github.com/spotify/flink-on-k8s-operator/blob/master/docs/savepoints_guide.md&quot;&gt;Flink operator savepoints guide&lt;/a&gt;. Of all the approaches listed, the easiest one I found to work for me was to &lt;strong&gt;annotate&lt;/strong&gt; the cluster manually.&lt;/p&gt;

&lt;p&gt;For example, given the flinkcluster resource above, I can run:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl annotate flinkclusters clickcount flinkclusters.flinkoperator.k8s.io/user-control&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;savepoint
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see the message &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;User control savepoint triggered&lt;/code&gt; event message and the savepoint shown under the job specs.&lt;/p&gt;

&lt;h4 id=&quot;saving-artifacts&quot;&gt;Saving artifacts&lt;/h4&gt;

&lt;p&gt;To save artifacts to S3 from batch jobs, we need to do the same as above:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a custom image where we copy over the hadoop s3 plugins&lt;/li&gt;
  &lt;li&gt;Create and mount the env vars&lt;/li&gt;
  &lt;li&gt;Specify the target s3 bucket with &lt;strong&gt;s3a&lt;/strong&gt; prefix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is an example of a batch job submitted to the flink operator:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flinkoperator.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;FlinkCluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pyflink-wordcount&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;flinkVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1.15.2&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m1l0/pyflink:1.15.2-scala_2.12&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pullPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IfNotPresent&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;taskManager&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Below is needed to access attached volumes as flink user&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;securityContext&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;runAsUser&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9999&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;runAsGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9999&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;fsGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9999&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;envVars&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ENABLE_BUILT_IN_PLUGINS&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-s3-fs-hadoop-1.15.2.jar;&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AWS_ACCESS_KEY_ID&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-aws-secret&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;access_key_id&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AWS_SECRET_ACCESS_KEY&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-aws-secret&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;secret_access_key&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pyFile&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;examples/python/datastream/word_count.py&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--output&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s3a://flinkexps/artifacts/pyflink/&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;restartPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Never&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;flinkProperties&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;s3.path.style.access&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above runs an example pyflink job provided in the Flink docker image which we customized by copying over the hadoop plugins. Note that the same env vars as specified for streaming jobs must be present for it to work in batch mode.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;This post attempts to explain how to create and store savepoints in remote cloud storage either through running a streaming job or as the artifacts of a batch job.&lt;/p&gt;

&lt;p&gt;In the next posts, I will attempt to cover checkpoints and the high availability mode in a flink cluster.&lt;/p&gt;

&lt;p&gt;H4ppy H4ck1n6 !!!&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/aws/s3/2022/11/10/flink-s3/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/aws/s3/2022/11/10/flink-s3/</guid>
      </item>
    
      <item>
        <title>Using Flink Kubernetes Operator for BEAM and pyflink workflows</title>
        <description>
&lt;p&gt;In the previous post, I described a process of using docker compose to setup a suite of services to develop and run BEAM jobs on a flink cluster. To mimic production scenarios, we can use a container orchestration platform such as &lt;strong&gt;kubernetes&lt;/strong&gt; to manage these services.&lt;/p&gt;

&lt;p&gt;In this post, I attempt to highlight the process of setting up a local &lt;strong&gt;kind&lt;/strong&gt; cluster to deploy and manage a flink session cluster using the &lt;a href=&quot;https://github.com/spotify/flink-on-k8s-operator&quot;&gt;Flink Kubernetes operator&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-the-kind-cluster&quot;&gt;Setting up the kind cluster&lt;/h3&gt;

&lt;p&gt;To setup a flink cluster in &lt;strong&gt;kind&lt;/strong&gt; we need to have kind installed locally.&lt;/p&gt;

&lt;p&gt;To create a kind cluster:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; kind_config.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The config file I used was documented on the website and as follows:
&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=kind_config.yaml&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;It exposes the host system ports 80 and 443 to allow the ingress controller to map those ports in order to access the UI which we will describe later.&lt;/p&gt;

&lt;p&gt;It also maps a local directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/artifacts&lt;/code&gt; into the node as type of host path. This can be referenced in pods with the path of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/artifacts&lt;/code&gt;. The local directory must be created with the owner and group of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9999&lt;/code&gt; before the flink task manager can read/write to it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /tmp/artifacts

&lt;span class=&quot;nb&quot;&gt;sudo chown&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; 9999:9999 /tmp/artifacts
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check that the cluster is running and set the kubectl context:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kind get clusters

kubectl cluster-info &lt;span class=&quot;nt&quot;&gt;--context&lt;/span&gt; kind-flinkcluster
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;2-setup-and-load-docker-images&quot;&gt;2. Setup and load docker images&lt;/h3&gt;

&lt;p&gt;Kind cluster doesn’t have access to the local docker images on the host system so they must be preloaded into the cluster. The following are some of the images we preload:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kind load docker-image apache/beam_python3.8_sdk:2.41.0 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster

kind load docker-image apache/flink:1.15.2 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster

kind load docker-image apache/beam_flink1.15_job_server:2.41.0 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster

kind load docker-image m1l0/pyflink:1.15.2-scala_2.12 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first 3 images will be used by the flink session cluster and job server. The last image is a custom image which contains both &lt;strong&gt;python&lt;/strong&gt; and &lt;strong&gt;pyflink&lt;/strong&gt; installed as the default flink image is designed to run in Java.&lt;/p&gt;

&lt;p&gt;The Dockerfile I used is as follows and adapted from &lt;a href=&quot;https://github.com/spotify/flink-on-k8s-operator&quot;&gt;Flink Kubernetes operator&lt;/a&gt;:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=Dockerfile.pyflink&quot;&gt;&lt;/script&gt;

&lt;p&gt;The dockerfile uses apache/flink:1.15.2-scala_2.12 as a base / builder image, installs some flink connectors such as mysql and hadoop and installs and builds python and pyflink.&lt;/p&gt;

&lt;h3 id=&quot;3-setup-flink-operator&quot;&gt;3. Setup Flink operator&lt;/h3&gt;

&lt;p&gt;To install the Flink operator, we need to install its dependencies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;certmanager&lt;/li&gt;
  &lt;li&gt;ingress nginx controller&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To install &lt;strong&gt;certmanager&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/jetstack/cert-manager/releases/download/v1.8.1/cert-manager.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next we install &lt;strong&gt;ingress nginx controller&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The ingress controller will create a ingress object for the flink dashboard.&lt;/p&gt;

&lt;p&gt;Next we install the flink operator:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/spotify/flink-on-k8s-operator/releases/download/v0.4.2-beta.4/flink-operator.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next we verify that the flink operator components are installed correctly:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl get crds | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;flinkclusters.flinkoperator.k8s.io

kubectl describe crds/flinkclusters.flinkoperator.k8s.io

kubectl get deployments &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; flink-operator-system

kubectl get pods &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; flink-operator-system

kubectl logs &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; flink-operator-system &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;flink-operator &lt;span class=&quot;nt&quot;&gt;--all-containers&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;4-create-flink-resources&quot;&gt;4. Create flink resources&lt;/h3&gt;

&lt;p&gt;We need to setup the following resources to create the cluster:&lt;/p&gt;

&lt;h4 id=&quot;4a-volumes&quot;&gt;4.a Volumes&lt;/h4&gt;

&lt;p&gt;We create two sets of volumes and persistent volume claims.&lt;/p&gt;

&lt;p&gt;We create a &lt;strong&gt;persistent volume&lt;/strong&gt; and a &lt;strong&gt;persistent volume claim&lt;/strong&gt; as the taskmanager and job server need to share a common staging space to access the uploaded artifacts, similar to the docker setup. This creates a volume on the node at path &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/mnt/data&lt;/code&gt;, mounted into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/beam-staging&lt;/code&gt; in the pod&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_pvc.yaml&quot;&gt;&lt;/script&gt;

&lt;p&gt;The second set of volume and claim creates a volume to store the job artifacts. When applied, if the output path for a job is specified as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/artifacts&lt;/code&gt;, the final output can also be accessed locally on the host.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_pvc_artifacts.yaml&quot;&gt;&lt;/script&gt;

&lt;p&gt;Applying the above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; flink_pvc.yaml

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; flink_pvc_artifacts.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;4b-flink-session-cluster-and-job-server&quot;&gt;4.b Flink session cluster and job server&lt;/h4&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_session_cluster.yaml&quot;&gt;&lt;/script&gt;

&lt;p&gt;The flink session cluster is based on a CRD &lt;strong&gt;FlinkCluster&lt;/strong&gt; defined by the flink operator. Note that in this example we are defining a &lt;strong&gt;session cluster&lt;/strong&gt; hence there is no &lt;strong&gt;job&lt;/strong&gt; spec defined. If &lt;strong&gt;job&lt;/strong&gt; spec is defined, the operator automatically provisions an &lt;strong&gt;application&lt;/strong&gt; cluster which runs the job once and exits.&lt;/p&gt;

&lt;p&gt;Another important note is that we are using the custom image we built earlier which includes python and pyflink. This is essential as pyflink jobs will not be able to run and fails with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python not found&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The python BEAM jobs are not affected by this as we are using the python beam sdk to submit the job which we will show later.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_ui_svc.yaml&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;flink_ui_svc.yaml&lt;/strong&gt; creates an Ingress resource in order to allow external traffic to the UI. Since we have exposed ports 80 in our kind cluster, we can access the UI with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost/default/ui&lt;/code&gt; in the browser.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_job_server.yaml&quot;&gt;&lt;/script&gt;

&lt;p&gt;Applying the above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; flink_session_cluster.yaml

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; flink_job_server.yaml

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; flink_ui_svc.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check that the resources are deployed correctly and running:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-lapp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;flink
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The pods should show status of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;running&lt;/code&gt; and the UI is accessible at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost/default/ui/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flink/operator/kubectl.png&quot; alt=&quot;Kubectl pods status&quot; /&gt;
&lt;img src=&quot;/assets/img/flink/operator/ui.png&quot; alt=&quot;Flink dashboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can test the current setup using the following example job:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=sample_test_job.yaml&quot;&gt;&lt;/script&gt;

&lt;p&gt;This defines an python beam job using the python sdk. Once the job is created, it gets submitted to the &lt;strong&gt;beam-job-server&lt;/strong&gt; service running on port 8099 which translates it into Flink compatible job. The environment_type and environment_config refers to the python BEAM sdk which Flink uses when it executes python code.&lt;/p&gt;

&lt;p&gt;The screenshot below shows the job completion in the UI:
&lt;img src=&quot;/assets/img/flink/operator/sample_job_ui.png&quot; alt=&quot;Flink dashboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Check in the UI and pod logs that the job has completed successfully.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl logs beam-wordcount-py-jpkqx 

INFO:root:Default Python SDK image &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;environment is apache/beam_python3.8_sdk:2.41.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:&lt;span class=&quot;o&quot;&gt;====================&lt;/span&gt; &amp;lt;&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;pack_combiners at 0x7fe9064305e0&amp;gt; &lt;span class=&quot;o&quot;&gt;====================&lt;/span&gt;
INFO:apache_beam.runners.portability.fn_api_runner.translations:&lt;span class=&quot;o&quot;&gt;====================&lt;/span&gt; &amp;lt;&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;lift_combiners at 0x7fe906430670&amp;gt; &lt;span class=&quot;o&quot;&gt;====================&lt;/span&gt;
INFO:apache_beam.runners.portability.fn_api_runner.translations:&lt;span class=&quot;o&quot;&gt;====================&lt;/span&gt; &amp;lt;&lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;sort_stages at 0x7fe906430dc0&amp;gt; &lt;span class=&quot;o&quot;&gt;====================&lt;/span&gt;
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The job can run on any of the taskmanagers which is referenced in the job UI. The outputs will be stored in the &lt;strong&gt;beam-worker-pool&lt;/strong&gt; sidecar container which can be accessed like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; beam-flink-cluster-taskmanager-0 &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; beam-worker-pool &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; bash
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/artifacts&lt;/code&gt; path is mounted into the cluster, the output can be accessed locally.&lt;/p&gt;

&lt;h3 id=&quot;5-running-custom-beam-jobs&quot;&gt;5. Running custom BEAM jobs&lt;/h3&gt;

&lt;p&gt;To run your own custom BEAM job we need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create a custom docker image using the apache python SDK image with the custom python files / modules loaded.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a kubernetes job spec with the above custom image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Submit the job into the kubernetes cluster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A dockerfile could look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c&quot;&gt;# Example dockerfile to build a custom image to run beam job&lt;/span&gt;
FROM apache/beam_python3.8_sdk:2.41.0 as builder

WORKDIR /opt/flink

COPY beam_example.py .
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here we use the &lt;strong&gt;apache/beam_python3.8_sdk:2.41.0&lt;/strong&gt; as the builder image and add our python beam code into the image.&lt;/p&gt;

&lt;p&gt;The sdk image has already been preloaded into the kind cluster earlier. Next we need to load this custom job image into the cluster:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kind load docker-image beam:custom &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A job manifest &lt;strong&gt;flink_beam_example.yaml&lt;/strong&gt; can look like this:
&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_beam_example.yaml&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To submit the job:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; flink_beam_example.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The job should appear in the UI:
&lt;img src=&quot;/assets/img/flink/operator/custom_sample_job.png&quot; alt=&quot;Flink dashboard&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6-running-custom-pyflink-job&quot;&gt;6. Running custom PyFlink job&lt;/h3&gt;

&lt;p&gt;Earlier we detailed how we use the &lt;strong&gt;apache/flink:1.15.2&lt;/strong&gt; image as a base image to build a custom image containing python, pyflink and other connectors. This is the same image we use to create and run our custom PyFlink jobs.&lt;/p&gt;

&lt;p&gt;This follows a similar process:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create a custom docker image using the custom apache flink image with the custom python files / modules loaded&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a kubernetes job spec with the above custom image&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Submit the job into the kubernetes cluster&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example dockerfile could be:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c&quot;&gt;# Example dockerfile to build a custom image to run pyflink job&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# NOTE: The pyflink image must have python else it fails with python not found and job fails?&lt;/span&gt;
FROM m1l0/pyflink:1.15.2-scala_2.12 as builder

WORKDIR /opt/flink

COPY pyflink_example.py .
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Create a job manifest as follows:
&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_pyflink_session_example.yaml&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Note that the &lt;strong&gt;beam-flink-cluster-jobmanager&lt;/strong&gt; is created as a service earlier. Here we run the custom pyflink file by using the &lt;strong&gt;/opt/flink/bin/flink&lt;/strong&gt; CLI tool, passing in the &lt;strong&gt;–python&lt;/strong&gt; argument which will cause the job to be parsed and submitted as a python code.&lt;/p&gt;

&lt;p&gt;Submit the job and monitor the UI for job status.
&lt;img src=&quot;/assets/img/flink/operator/custom_pyflink_job.png&quot; alt=&quot;Flink dashboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Access the job artifacts by execing into the &lt;strong&gt;taskmanager&lt;/strong&gt; pod and accessing the &lt;strong&gt;taskmanager&lt;/strong&gt; container since the job is run directly on the flink cluster this time.&lt;/p&gt;

&lt;h3 id=&quot;7-running-custom-pyflink-job-in-application-mode&quot;&gt;7. Running custom PyFlink job in Application Mode&lt;/h3&gt;

&lt;p&gt;The operator supports running jobs in either &lt;strong&gt;session&lt;/strong&gt; or &lt;strong&gt;application&lt;/strong&gt; mode. The steps before detailed job submission via the session mode to a standalone flink cluster.&lt;/p&gt;

&lt;p&gt;When run in application mode, the operator creates an individual flink cluster with job and task managers to execute the job.&lt;/p&gt;

&lt;p&gt;An example PyFlink application job:
&lt;script src=&quot;https://gist.github.com/cheeyeo/fbbda6ecc4d94f14bba57c36276d6edd.js?file=flink_pyflink_job_example.yaml&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flink/operator/application_pyflink.png&quot; alt=&quot;Flink Application CLI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The operator creates a standalone flink cluster comprising of a jobmanager, taskmanager to run the job. It also creates 2 jobs: the actual job defined in the spec, and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;submitter&lt;/code&gt; job which uses the flink CLI tool to submit the job and reports on it status and logs.&lt;/p&gt;

&lt;p&gt;To view the status and logs of the job:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kubectl describe pod pyflink-wordcount-cluster-job-submitter-tn5zz
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Sample output of the logs:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;...

    State:      Terminated
      Reason:   Completed
      Message:  jobID: 85cda7250d75e94dee8586f90f757871
message: |
  Successfully submitted!
  /opt/flink/bin/flink run &lt;span class=&quot;nt&quot;&gt;--jobmanager&lt;/span&gt; pyflink-wordcount-cluster-jobmanager:8081 &lt;span class=&quot;nt&quot;&gt;--parallelism&lt;/span&gt; 1 &lt;span class=&quot;nt&quot;&gt;--detached&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--python&lt;/span&gt; examples/python/datastream/word_count.py &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; /artifacts/application
  WARNING: An illegal reflective access operation has occurred
  WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;file:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; to method sun.security.krb5.Config.getInstance&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
  WARNING: Use &lt;span class=&quot;nt&quot;&gt;--illegal-access&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;warn to &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;warnings of further illegal reflective access operations
  WARNING: All illegal access operations will be denied &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;a future release
  Job has been submitted with JobID 85cda7250d75e94dee8586f90f757871
  Executing word_count example with default input data set.
  Use &lt;span class=&quot;nt&quot;&gt;--input&lt;/span&gt; to specify file input.

...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Sample of the generated output:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;a,5&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Be,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Is,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;No,2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Or,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;To,4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;be,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;by,2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;he,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;,3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;is,2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;my,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;of,14&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;or,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;so,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;to,7&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;us,3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;we,4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;And,5&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;But,1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;cleanup&quot;&gt;Cleanup&lt;/h3&gt;

&lt;p&gt;One can delete the entire kind cluster using the following to remove all resources:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;kind delete cluster &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; flinkcluster
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The post attempts to explain how to run a Flink cluster locally in a &lt;strong&gt;kind&lt;/strong&gt; cluster using the Flink operator.&lt;/p&gt;

&lt;p&gt;Using the operator has simplified the process of managing and applying various disparate kubernetes config files which is error prone. Defining a flink cluster and job as custom resource definition also simplifies resource management and further interoperability with the kubernetes API.&lt;/p&gt;

&lt;p&gt;The downside of this approach is it involves more automation in order to package and deploy custom jobs locally into the cluster. I see this particular setup as more of a pre-deployment or test environment of a BEAM/Flink job before running it in an actual cluster remotely.&lt;/p&gt;

&lt;p&gt;These are the remaining areas which remains to be researched on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install operator using helm chart&lt;/li&gt;
  &lt;li&gt;Custom application to create a job resource dynamically&lt;/li&gt;
  &lt;li&gt;Use of savepoints to save and restore running jobs&lt;/li&gt;
  &lt;li&gt;Use of connectors such as S3 to save artifacts and savepoints remotely.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;H4ppy H4ck1n6 !!!&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/2022/11/10/flink-operator/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/2022/11/10/flink-operator/</guid>
      </item>
    
      <item>
        <title>Setting up Flink cluster for BEAM workflow</title>
        <description>
&lt;p&gt;&lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt; allows one to create reusable, chainable pipelines for data processing tasks. While it is possible to run BEAM python jobs directly using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DirectRunner&lt;/code&gt; without a backend, in production environments, we tend to use a backend runner such as Spark or &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This article will try to explain how to setup and run a local &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; cluster to run BEAM jobs on. There are two different scenarios: running &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt; through a set of Docker containers; and running Flink on a local kubernetes cluster. This article will explain and demonstrate how to do so using Docker. A subsequent article will go into detail on how to deploy a Flink cluster in Kubernetes using Kind cluster locally.&lt;/p&gt;

&lt;p&gt;The main components of a Flink cluster are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Job Manager&lt;/p&gt;

    &lt;p&gt;The core component of a Flink cluster. It serves as the control plane of the cluster and coordinates work submitted to the cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task Manager&lt;/p&gt;

    &lt;p&gt;Component that performs / executes the work of a Flink job from the Job Manager.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Out of the box, Flink supports pipelines written in Java. For other languages such as Python or Go LANG, we need to submit the job as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PortableRunner&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This requires running 2 additional components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Job Server&lt;/p&gt;

    &lt;p&gt;This is where the pipeline gets submitted from the Python SDK via the Job API. Beam converts it to Runner API before submitting the pipeline via Job API to Beam’s job server.&lt;/p&gt;

    &lt;p&gt;The Job Server translates the pipeline code into a compatible Flink program and submits it to the Flink cluster for execution.&lt;/p&gt;

    &lt;p&gt;The compiled pipeline code is a Flink program that contains an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ExecutableStage&lt;/code&gt; transform, which is a ParDo transform designed for holding language dependent code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Python SDK Harness&lt;/p&gt;

    &lt;p&gt;This is the language specific &lt;strong&gt;environment&lt;/strong&gt; where the target pipeline is executed after its submitted to the Flink cluster.&lt;/p&gt;

    &lt;p&gt;When Flink executes Python code, it sends data to the Python environment containing the Python SDK harness.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example docker compose config for running Flink cluster locally in session mode:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3.9&quot;&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;flink-job-artifacts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-job-artifacts&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;jobmanager&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink:1.15.2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;expose&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6123&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6124&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;8081:8081&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jobmanager&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;./conf:/opt/flink/conf&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/flink-checkpoints-directory:/tmp/flink-checkpoints-directory&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/flink-savepoints-directory:/tmp/flink-savepoints-directory&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;JOB_MANAGER_RPC_ADDRESS=localhost&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;network_mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;taskmanager&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink:1.15.2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;expose&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6121&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6122&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6125&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jobmanager&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;taskmanager&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;./conf:/opt/flink/conf&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/flink-checkpoints-directory:/tmp/flink-checkpoints-directory&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/flink-savepoints-directory:/tmp/flink-savepoints-directory&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-job-artifacts:/artifacts:rw&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../outputs:/outputs:rw&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# mounts local output dir&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;JOB_MANAGER_RPC_ADDRESS=localhost&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;network_mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;
  

  &lt;span class=&quot;c1&quot;&gt;# BEAM Job Server&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# e.g. if pip apache-beam == 2.41.0, the image must be the same&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;jobserver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apache/beam_flink1.15_job_server:2.41.0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;8099:8099&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-job-artifacts:/artifacts:rw&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../data:/data:rw&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# data source from localhost&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../outputs:/outputs:rw&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# mounts local output dir&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--artifacts-dir&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/artifacts&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--flink-master-url&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;localhost:8081&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--clean-artifacts-per-job&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;network_mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;
  
  &lt;span class=&quot;c1&quot;&gt;# Specifies a python runner environment&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# SDK tag version must match the pip installed apache-beam version&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# e.g. if pip apache-beam == 2.41.0, the image must be the same&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;python_sdk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apache/beam_python3.8_sdk:2.41.0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jobmanager&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;taskmanager&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;container_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;beam-python-sdk&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--worker_pool&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;50000:50000&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flink-job-artifacts:/artifacts:rw&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../data:/data:rw&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# data source from localhost&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../outputs:/outputs:rw&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# mounts local output dir&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;network_mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We declare a volumes of &lt;strong&gt;flink-job-artifacts&lt;/strong&gt; store the output of the translated pipeline before submission to the Flink server. Both the taskmanager and job server must share the same path in order for the taskmanager to pick up the compiled pipeline.&lt;/p&gt;

&lt;p&gt;Next we run a job server where the actual BEAM code is submitted to.&lt;/p&gt;

&lt;p&gt;Note that the version of &lt;strong&gt;apache-beam&lt;/strong&gt; installed locally in pip must match that of the sdk and job server. e.g. if we have &lt;strong&gt;python 3.8&lt;/strong&gt; and &lt;strong&gt;apache-beam==2.41.0&lt;/strong&gt; installed locally, we need the &lt;strong&gt;apache/beam_flink1.15_job_server:2.41.0&lt;/strong&gt; and &lt;strong&gt;apache/beam_python3.8_sdk:2.41.0&lt;/strong&gt; images.&lt;/p&gt;

&lt;p&gt;Note that we are using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;network_mode: host&lt;/code&gt;, which means it uses the underlying host’s network settings rather than the docker engine. This is required for the various services to communicate as there is no service discovery and the urls are hardcoded to localhost within the Flink JAR files.&lt;/p&gt;

&lt;p&gt;The above cluster runs in session mode which means we can have multiple task managers by increasing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scale&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;To run:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;  docker compose &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; docker-compose-portable.yml up
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check that the console is running by going to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:8081&lt;/code&gt;
&lt;img src=&quot;/assets/img/flink/dashboard.png&quot; alt=&quot;Flink dashboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can create a simple Beam python pipeline like below to test:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;logging&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;apache_beam&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;apache_beam.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ReadFromText&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;apache_beam.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WriteToText&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;apache_beam.options.pipeline_options&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PipelineOptions&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Path to save output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;known_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline_args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_known_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pipeline_options&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PipelineOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;Create words&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;to be or not to be&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;Split words&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FlatMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos; &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;Write to file&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WriteToText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;known_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getLogger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setLevel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Without going into how BEAM works, which is left as an exercise to the reader, the above pipeline creates an initial string which is passed to a transform that splits up the words, and finally to another transform that writes the words into a file sink.&lt;/p&gt;

&lt;p&gt;The pipeline options are also created via the argparse lib. Structuring the pipeline above allows for running locally in dev mode using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DirectRunner&lt;/code&gt; and also &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PortableRunner&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To run the above pipeline:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;python beam_examples/simple_example.py &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--job_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;SimpleExample &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;PortableRunner &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--environment_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;EXTERNAL &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--environment_config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;localhost:50000 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--job_endpoint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;localhost:8099 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; /outputs/NEW_FILE.txt
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flink/cli.png&quot; alt=&quot;Running BEAM job in CLI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can also view the job progress via the dashboard
&lt;img src=&quot;/assets/img/flink/dashboard-job.png&quot; alt=&quot;Job in Dashboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;job_endpoint&lt;/code&gt; is set to the job server service defined in the above compose config. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;environment_config&lt;/code&gt; refers to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python-sdk&lt;/code&gt; which is attached at port 50000 to the task manager.&lt;/p&gt;

&lt;p&gt;The above example has mounted a local directory of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;../outputs&lt;/code&gt; to store the results of the pipeline.&lt;/p&gt;

</description>
        <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/datascience/dataprocessing/beam/flink/2022/11/01/flink-beam/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/datascience/dataprocessing/beam/flink/2022/11/01/flink-beam/</guid>
      </item>
    
      <item>
        <title>Saving Model checkpoint in Tensorflow 2.0 using tf.train.Checkpoint</title>
        <description>
&lt;p&gt;Whilst exploring how to build and train a GAN model in tensorflow I came upon an interesting issue on how to save the model’s checkpoints during training.&lt;/p&gt;

&lt;p&gt;One can usually save a model’s checkpoint either using the built-in &lt;strong&gt;ModelCheckpoint&lt;/strong&gt; callback or by using a custom callback class subclassing from &lt;strong&gt;Callback&lt;/strong&gt;. The issue is with a GAN, we have two models being trained concurrently - the critic / discriminator and the generator.&lt;/p&gt;

&lt;p&gt;Normally, we tend to wrap both models inside another class object or we can create a custom model class subclassed from &lt;strong&gt;tf.keras.models.Model&lt;/strong&gt; and override the &lt;strong&gt;train_step&lt;/strong&gt; function.&lt;/p&gt;

&lt;p&gt;In both cases, we can’t define a callback on the resultant model using &lt;strong&gt;fit&lt;/strong&gt; as it is only a logical wrapper around the critic/generator models.&lt;/p&gt;

&lt;p&gt;If we attempt to define the checkpoint callback on the logical wrapper model we will get the following error:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcgan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DCGAN&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x7f84463f9d10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cannot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;saved&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;because&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Usually&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;automatically&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;determined&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calling&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`.fit()`&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`.predict()`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;To&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;manually&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;call&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`model.build(input_shape)`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As per the &lt;a href=&quot;https://www.tensorflow.org/tutorials/generative/dcgan&quot;&gt;TF 2.0 Guide on training GAN&lt;/a&gt;, it uses an object of &lt;strong&gt;tf.train.Checkpoint&lt;/strong&gt; to save the checkpoints of the optimizers, generator and critic models during training.&lt;/p&gt;

&lt;p&gt;Using the above approach, we can create a custom callback that would allow us to save the current checkpoint per epoch:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;EpochCheckpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;every&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ckpt_obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EpochCheckpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;every&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;every&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ckpt_obj&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_epoch_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;every&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;checkpoint_prefix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ckpt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_prefix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_prefix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Firstly, we subclass &lt;strong&gt;Callback&lt;/strong&gt; class and initialize some instance variables:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;checkpoint_dir&lt;/strong&gt; where the checkpoint is to be saved&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;every&lt;/strong&gt;, frequency at which we save per epoch&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;int_epoch&lt;/strong&gt;, when to start saving the checkpoint, Defaults to epoch 0&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;checkpoint&lt;/strong&gt;, the &lt;strong&gt;tf.train.Checkpoint&lt;/strong&gt; object which gets passed from the training script containing the optimizers and models to save&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We override the &lt;strong&gt;on_epoch_end&lt;/strong&gt; function to save the checkpoints at the end of each epoch. Within the function call, we initialize the prefix to save the checkpoint and calls the &lt;strong&gt;save&lt;/strong&gt; method of the checkpoint object. Then we increment the internal counter to track the current epoch number.&lt;/p&gt;

&lt;p&gt;Within the main training script, we need to initialize the above callback and define the objects we want the checkpoint to store. If training is interrupted, we need a way to resume training from the last saved checkpoint. This can be accomplished by calling &lt;strong&gt;tf.train.latest_checkpoint&lt;/strong&gt;, passing in the checkpoint directory. If any checkpoints exist, it will return the filepath else it returns None.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;ckpt_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;checkpoints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# when to start checkpoint; will be 0 when first training
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Define the objects we want TF to track
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ckpt_obj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d_opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g_opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;discriminator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;discriminator&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;latest_ckpt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latest_checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ckpt_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latest_ckpt&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[INFO] Resuming from ckpt: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latest_ckpt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ckpt_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;restore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latest_ckpt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expect_partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;latest_ckpt_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latest_ckpt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latest_ckpt_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Resuming ckpt at &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ckpt_callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EpochCheckpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ckpt_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;every&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ckpt_obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ckpt_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EPOCHS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ckpt_callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;From above, we define the checkpoint directory. Next we create a &lt;strong&gt;tf.train.Checkpoint&lt;/strong&gt; object. This allows us to define the objects we wish to track using a dictionary. In this case, we define the two optimizers and the generator and critic models.&lt;/p&gt;

&lt;p&gt;Next we check if we are resuming training from previous checkpoint by calling &lt;strong&gt;tf.train.latest_checkpoint&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If this is the first time we are running the training script, there will be no checkpoints and this will return None. The script will continue to call fit and start training from scratch.&lt;/p&gt;

&lt;p&gt;If there are any checkpoints found, it will call &lt;strong&gt;restore&lt;/strong&gt; on the checkpoint object and attempt to extract the epoch number from its filepath. This sets the &lt;strong&gt;start_at&lt;/strong&gt; variable which gets passed into the callback object to resume training from that specific checkpoint found.&lt;/p&gt;

&lt;p&gt;Full working example can be found on &lt;a href=&quot;https://www.tensorflow.org/tutorials/generative/dcgan&quot;&gt;TF 2.0 Guide on training GAN&lt;/a&gt; and the official &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint&quot;&gt;tf.train.Checkpoint API&lt;/a&gt; has implementation examples.&lt;/p&gt;

&lt;p&gt;Happy Hacking !!!&lt;/p&gt;
</description>
        <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/machine-learning/tensorflow/tf2.0/gan/2022/01/04/tensorflow-checkpoint/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/machine-learning/tensorflow/tf2.0/gan/2022/01/04/tensorflow-checkpoint/</guid>
      </item>
    
      <item>
        <title>Fixing thunderbolt3 on Ubuntu 18.04 LTS for eGPU</title>
        <description>
&lt;p&gt;On a previous post on using &lt;a href=&quot;https://www.cheeyeo.dev/egpu/ubuntu/18.04/machine-learning/2020/03/13/multi-egpu-ubuntu/&quot;&gt;eGPU for Machine Learning&lt;/a&gt;, I described a process of setting up an external GPU for local distributed training of ML models.&lt;/p&gt;

&lt;p&gt;It relies on using the &lt;strong&gt;bolt&lt;/strong&gt; package provided upstream which is fixed at &lt;strong&gt;0.5.0&lt;/strong&gt; for Ubuntu 18.04 LTS.&lt;/p&gt;

&lt;p&gt;After a kernel update to version &lt;strong&gt;4.15.0-163-generic&lt;/strong&gt;, the thunderbolt controller was put into a forced shutdown everytime the system starts, resulting in the thunderbolt3 controller not being able to recognise the eGPU.&lt;/p&gt;

&lt;p&gt;After some digging, I discovered that the thunderbolt controller is running as a background service under systemctl. This means that we can stop this service and replace it with another one which calls an updated version of bolt.&lt;/p&gt;

&lt;p&gt;I removed the system version and resinstalled a later version of boltd to test if this would fix the issue of the forced shutdown detailed above.&lt;/p&gt;

&lt;p&gt;The process I took was as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Download the &lt;a href=&quot;https://gitlab.freedesktop.org/bolt/bolt/-/releases&quot;&gt;bolt source&lt;/a&gt;. I picked version 0.7.0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Setup a venv through python as the build stage requires meson, a python package, for compilation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;make sure &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;virtual &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;and &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;meson and ninja 
&lt;span class=&quot;go&quot;&gt;python3 -m venv boltvenv
source boltvenv/bin/activate
pip install meson
pip install ninja

sudo apt-get install libpolkit-gobject-1-dev&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Compile bolt from source as follows:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;go&quot;&gt;curl -L https://gitlab.freedesktop.org/bolt/bolt/-/archive/0.7/bolt-0.7.tar.gz -o bolt.tar.gz

tar -zxvf bolt.tar.gz

cd bolt-0.7.0

meson build \
      --sysconfdir=/etc \
      --localstatedir=/var \
      --sharedstatedir=/var/lib&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that I left the build parameters as the defaults as documented on the project website.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build the bolt package:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;go&quot;&gt;ninja -C build

ninja -C build test

sudo ninja -C build install

sudo systemctl daemon-reload&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Restart the system.&lt;/p&gt;

&lt;p&gt;If the above goes well, you should have the bolt service running after running &lt;strong&gt;sudo systemctl status bolt&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/egpu/bolt_systemctl.png&quot; alt=&quot;Output of systemctl status for bolt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Running &lt;strong&gt;boltctl –version&lt;/strong&gt; should also report version 0.7.0&lt;/p&gt;

&lt;p&gt;Next check that the current device (laptop) is registered successfully as a domain under bolt via :&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;go&quot;&gt;boltctl domains&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/egpu/bolt_domains.png&quot; alt=&quot;Output of boltctl domains&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Connect the eGPU, reboot and check that its connected and registered with bolt:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;go&quot;&gt;boltctl list&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/egpu/bolt_list.png&quot; alt=&quot;Output of boltctl list&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/egpu/gnome_panel.png&quot; alt=&quot;GNOME Panel view of thunderbolt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the screenshots above show that the eGPU has been purposefully disconnected to ensure that the device has been successfully registered on system reboot. Using version 0.5.0 of bolt package, it was blank.&lt;/p&gt;

&lt;p&gt;The screenshots also shows that it has managed to register the eGPU on reboot. The GNOME control panel has also displayed the registered eGPU which shows that the bolt service is running properly.&lt;/p&gt;

&lt;p&gt;Note that you will need to plugin the eGPU and restart before &lt;strong&gt;nvidia-smi&lt;/strong&gt; can recognise the additional GPU.&lt;/p&gt;

&lt;h3 id=&quot;further-notes&quot;&gt;Further Notes&lt;/h3&gt;

&lt;p&gt;If the eGPU is still not recognised you can check that the physical thunderbolt controller is actually enabled under the BIOS.&lt;/p&gt;

&lt;p&gt;I ran the following command from the terminal since I can’t get into BIOS due to GRUB:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;go&quot;&gt;sudo systemctl reboot --firmware-setup&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check for the presence of thunderbolt3 hardware controller and ensure its enabled.&lt;/p&gt;

&lt;p&gt;As an additional measure, I also enabled the option to detect thunderbolt devices with PCIe cards installed to be initiated on startup, if its available as an option in your BIOS settings.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The thunderbolt3 controller in Ubuntu runs as a background service of &lt;strong&gt;boltd&lt;/strong&gt; which can be upgraded manually to a later version to address issues of force shutdowns. Note that I only tested version 0.7.0. since its the only version I can get to work with the GNOME control panel.&lt;/p&gt;

&lt;p&gt;Happy hacking !!!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/machine-learning/thunderbolt3/egpu/ubuntu/18.04/2021/11/23/thunderbolt3-egpu-ubuntu/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/machine-learning/thunderbolt3/egpu/ubuntu/18.04/2021/11/23/thunderbolt3-egpu-ubuntu/</guid>
      </item>
    
      <item>
        <title>Guide to training your own object detector using the TFOD API V2</title>
        <description>
&lt;p&gt;In my recent studies on computer vision, I come across the Faster-RCNN network, which is widely used in real-time object detection.&lt;/p&gt;

&lt;p&gt;The purpose of this post is to describe how to get up and running with the TFOD framework. I defer a detailed discussion of the Faster-RCNN architecture and how to evaluate and export the trained model in a follow-up post.&lt;/p&gt;

&lt;h3 id=&quot;hardware-and-os-tested&quot;&gt;Hardware and OS tested&lt;/h3&gt;

&lt;p&gt;Tested on Ubuntu 18.04 LTS with a single GeForce GTX 1060 GPU, 16GB RAM.&lt;/p&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;An &lt;a href=&quot;https://github.com/cheeyeo/tfod_rcnn_example&quot;&gt;example TFOD project&lt;/a&gt; is created with this blog post to highlight the process.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html&quot;&gt;LISA Traffic signs dataset&lt;/a&gt; is used for training and evaluation.&lt;/p&gt;

&lt;p&gt;The dataset consists of 47 different USA traffic sign types. There are 7855 individual annotations. The training images were taken from a dashcam footage with varying levels of quality and resolution.&lt;/p&gt;

&lt;p&gt;To limit the amount of resources required for training, we will only be using 3 traffic sign types for training: &lt;strong&gt;stop sign, pedestrian crossing, signal ahead signs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The dataset will need to be preprocessed into a specific format, namely the TF record format, before it can be used by the TFOD API.&lt;/p&gt;

&lt;p&gt;Also the class labels will need to be processed into a specific format before it can be used.&lt;/p&gt;

&lt;h3 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h3&gt;

&lt;p&gt;As mentioned previously, the input data needs to be converted into &lt;strong&gt;tf.train.Example&lt;/strong&gt; record with details of each input converted into a &lt;strong&gt;tf.train.Features&lt;/strong&gt; object.&lt;/p&gt;

&lt;p&gt;The dataset has 47 categories. We are only using three of it and defined it in the project config file as a python dict:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pedestrianCrossing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;signalAhead&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;stop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, we need to convert the dict above into the required format:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;pedestrianCrossing&apos;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;signalAhead&apos;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;stop&apos;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above is implemented in the &lt;strong&gt;build_lisa_records.py&lt;/strong&gt; script in the &lt;a href=&quot;https://github.com/cheeyeo/tfod_rcnn_example&quot;&gt;example TFOD project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Each image detail is stored as a CSV row in the &lt;strong&gt;allAnnotations.csv&lt;/strong&gt; file in the following format:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Filename&lt;/li&gt;
  &lt;li&gt;Annotation tag ( class label )&lt;/li&gt;
  &lt;li&gt;Upper left corner X ( start X )&lt;/li&gt;
  &lt;li&gt;Upper left corner Y ( start Y )&lt;/li&gt;
  &lt;li&gt;Lower right corner X ( end X )&lt;/li&gt;
  &lt;li&gt;Lower right corner Y ( end Y )&lt;/li&gt;
  &lt;li&gt;Occluded,On another road ( not used )&lt;/li&gt;
  &lt;li&gt;Origin file ( not used )&lt;/li&gt;
  &lt;li&gt;Origin frame number ( not used )&lt;/li&gt;
  &lt;li&gt;Origin track ( not used )&lt;/li&gt;
  &lt;li&gt;Origin track frame number ( not used )&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We parse the above CSV file, ignoring the headers and only use the first 6 fields to obtain the filename, label, and bounding box coordinates. The parsed data is stored into a python dict, keyed by the image filename. Each value is a tuple of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;((label, (startX, startY, endX, endY)))&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The dictionary keys are passed through to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_test_split&lt;/code&gt; in scikit-learn to split the dataset into train/test sets. We use a split of 0.75 for training and 0.25 for testing.&lt;/p&gt;

&lt;p&gt;For each of the data split, we need to convert each entry into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.train.Example&lt;/code&gt; record with the following required fields for its features, which is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.train.Features&lt;/code&gt; object:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“image/height”&lt;/li&gt;
  &lt;li&gt;“image/width”&lt;/li&gt;
  &lt;li&gt;“image/filename”&lt;/li&gt;
  &lt;li&gt;“image/source_id” ( filename )&lt;/li&gt;
  &lt;li&gt;“image/encoded” ( actual image encoded into bytes )&lt;/li&gt;
  &lt;li&gt;“image/format” ( image file type )&lt;/li&gt;
  &lt;li&gt;“image/object/bbox/xmin” ( start X coord of bounding box ground truth )&lt;/li&gt;
  &lt;li&gt;“image/object/bbox/xmax” ( end X coord of bounding box ground truth )&lt;/li&gt;
  &lt;li&gt;“image/object/bbox/ymin” ( start Y coord of bounding box ground truth )&lt;/li&gt;
  &lt;li&gt;“image/object/bbox/ymax” ( end Y coord of bounding box ground truth )&lt;/li&gt;
  &lt;li&gt;“image/object/class/text” ( string class label )&lt;/li&gt;
  &lt;li&gt;“image/object/class/label” ( integer class label )&lt;/li&gt;
  &lt;li&gt;“image/object/difficult”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that for “image/encoded”, we read each image using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.io.gfile.GFile&lt;/code&gt; and encode it into bytes.&lt;/p&gt;

&lt;p&gt;Note that each of the bounding box coordinate is also normalized to the range [0, 1] by dividing it by its width and height values.&lt;/p&gt;

&lt;p&gt;Note that for “image/object/class/label”, we refer to the python dict for the classes defined in the config file to obtain an integer representation of the string label.&lt;/p&gt;

&lt;p&gt;A custom class &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TFAnnotation&lt;/code&gt; is created to generate the above features which are then used to create an individual example object:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TFRecordWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfannot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SerializeToString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above is implemented in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build_lisa_records.py&lt;/code&gt; script.&lt;/p&gt;

&lt;p&gt;After preprocessing, we should obtain the following training files:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;lisa/records/
├── classes.pbtxt
├── testing.record
└── training.record&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;setup-tfod&quot;&gt;Setup TFOD&lt;/h3&gt;

&lt;p&gt;To setup TFOD we need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clone the models repository&lt;/li&gt;
  &lt;li&gt;Generate the protobuf files&lt;/li&gt;
  &lt;li&gt;Copy the setup.py file provided and install the dependencies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above can be summarized as follows:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;cd /tfod_example

git clone https://github.com/tensorflow/models.git

cd models/research/

protoc object_detection/protos/*.proto --python_out=.

cp object_detection/packages/tf2/setup.py .

python -m pip install --use-feature=2020-resolver .

cd ../../

setup.sh

python object_detection/builders/model_builder_tf2_test.py&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.sh&lt;/code&gt; script is needed to add the absolute path of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;models/research&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;models/research/slim&lt;/code&gt; directories to PYTHONPATH in order for the training script to locate the imports.&lt;/p&gt;

&lt;p&gt;An example setup.sh script looks as follows:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;/bin/sh
&lt;span class=&quot;go&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;export PYTHONPATH=$&lt;/span&gt;PYTHONPATH:&lt;span class=&quot;s1&quot;&gt;&apos;/tfod_example/models/research&apos;&lt;/span&gt;:&lt;span class=&quot;s1&quot;&gt;&apos;/tfod_example/models/research/slim&apos;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the provided test script runs successfully, you will have setup TFOD.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/cheeyeo/tfod_rcnn_example&quot;&gt;example TFOD project&lt;/a&gt; has a setup.sh script provided that takes as arguments the required paths and set them up as environment variables.&lt;/p&gt;

&lt;h3 id=&quot;setup-pre-trained-model&quot;&gt;Setup pre-trained model&lt;/h3&gt;

&lt;p&gt;I decided to use the pre-trained &lt;a href=&quot;http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8.tar.gz&quot;&gt;Faster R-CNN Resnet101 V1 model&lt;/a&gt; for this example.&lt;/p&gt;

&lt;p&gt;Faster-RCNN describes an architecture whereby a pre-trained base model is used for transfer learning. In this case, we are using ResNet but you can swap it out for other network types such as MobileNet for example.&lt;/p&gt;

&lt;p&gt;Download the &lt;a href=&quot;http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8.tar.gz&quot;&gt;Faster R-CNN Resnet101 V1 model&lt;/a&gt; and untar it in a specific directory:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;mkdir -p lisa/experiments/training

tar -zxvf faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8.tar.gz&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that each model archive has an entry of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpu&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tpu&lt;/code&gt; in its filename. You need to select the model based on your own hardware. For instance, for training using Colab, the tpu version should be used. If you have a local gpu to train against, use the gpu version.&lt;/p&gt;

&lt;p&gt;The model dir will consist of the following structure:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;lisa/experiments/training/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8/
├── checkpoint
│   ├── checkpoint
│   ├── ckpt-0.data-00000-of-00001
│   └── ckpt-0.index
├── saved_model
│   ├── variables
│   │   ├── variables.data-00000-of-00001
│   │   └── variables.index
│   └── saved_model.pb
└── pipeline.config&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This directory contains the pretrained model’s weights and a sample config file. The path to the model’s weights is referenced in the training config file below.&lt;/p&gt;

&lt;p&gt;We make a copy of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline.config&lt;/code&gt; file and use it as a starting point for this project.&lt;/p&gt;

&lt;h3 id=&quot;defining-the-training-config&quot;&gt;Defining the training config&lt;/h3&gt;

&lt;p&gt;After making a copy of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline.config&lt;/code&gt; file above, we need to update it with our custom paths as follows. For this example we rename the sample config file as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;faster_rcnn_lisa.config&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For the model config, we set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_classes&lt;/code&gt; to 3 specific for this example.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;faster_rcnn&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image_resizer&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;keep_aspect_ratio_resizer&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;min_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;....&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_config&lt;/code&gt; block we update the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_steps&lt;/code&gt;, and checkpoints parameters:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;num_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;fine_tune_checkpoint_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;fine_tune_checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/experiments/training/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8/checkpoint/ckpt-0&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;from_detection_checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;fine_tune_checkpoint_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We set the num of training steps to 50000. The original value is 200000 but you would need a minimum of at least 20000 training steps for a baseline model. The batch size is set to 1 for my setup but can be increased to match your existing compute resources. The important configuration are the checkpoint values. Note that the &lt;strong&gt;fine_tune_checkpoint&lt;/strong&gt; path must point to the checkpoint file in the model download from before. The filename should be just the prefix i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ckpt-0&lt;/code&gt;. Also set the finetune checkpoint type to “detection”.&lt;/p&gt;

&lt;p&gt;The next config blocks to change would be for the training and test datasets.&lt;/p&gt;

&lt;p&gt;For the train_input_reader block:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_input_reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;label_map_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/records/classes.pbtxt&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tf_record_input_reader&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/records/training.record&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that these files are generated during the data preprocessing phase described above. The &lt;strong&gt;label_map_path&lt;/strong&gt; refers to the mapping of string class names to its integer value. The &lt;strong&gt;tf_record_input_reader&lt;/strong&gt; is the TFRecord training file.&lt;/p&gt;

&lt;p&gt;Next we update the eval_input_reader block:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;eval_input_reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;label_map_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/records/classes.pbtxt&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tf_record_input_reader&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/records/testing.record&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;tf_record_input_reader&lt;/strong&gt; refers to the TFRecord for the test set.&lt;/p&gt;

&lt;p&gt;For the eval_config block:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;eval_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;metrics_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;coco_detection_metrics&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;num_examples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;955&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We set the num_examples to match the total number of examples in the test set which is provided when running the preprocessing script.&lt;/p&gt;

&lt;p&gt;Note that all the paths must be absolute paths.&lt;/p&gt;

&lt;h3 id=&quot;training-process&quot;&gt;Training process&lt;/h3&gt;

&lt;p&gt;The final step is to run the provided training script from within the TFOD models directory cloned earlier.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;research&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;object_detection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_main_tf2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; \
	&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_config_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/experiments/training/faster_rcnn_lisa.config&quot;&lt;/span&gt; \
	&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tfod_example/lisa/experiments/training&quot;&lt;/span&gt; \
	&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train_steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt; \
	&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_1_of_n_eval_examples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; \
	&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alsologtostderr&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You need to setup the &lt;strong&gt;PYTHONPATH&lt;/strong&gt; for the TFOD imports before runnning the training script by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source setup.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that we are using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_main_tf2.py&lt;/code&gt; script as this example is for TF 2.&lt;/p&gt;

&lt;p&gt;If training starts successfully, you should see the following output:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;c&quot;&gt;...
&lt;/span&gt;&lt;span class=&quot;go&quot;&gt;
INFO:tensorflow:Step 2100 per-step time 0.733s
I1103 12:46:44.766502 140491317245760 model_lib_v2.py:700] Step 2100 per-step time 0.733s
INFO:tensorflow:{&apos;Loss/BoxClassifierLoss/classification_loss&apos;: 0.01688414,
 &apos;Loss/BoxClassifierLoss/localization_loss&apos;: 0.05560676,
 &apos;Loss/RPNLoss/localization_loss&apos;: 0.013312755,
 &apos;Loss/RPNLoss/objectness_loss&apos;: 0.008382628,
 &apos;Loss/regularization_loss&apos;: 0.0,
 &apos;Loss/total_loss&apos;: 0.09418628,
 &apos;learning_rate&apos;: 0.0042}
I1103 12:46:44.766725 140491317245760 model_lib_v2.py:701] {&apos;Loss/BoxClassifierLoss/classification_loss&apos;: 0.01688414,
 &apos;Loss/BoxClassifierLoss/localization_loss&apos;: 0.05560676,
 &apos;Loss/RPNLoss/localization_loss&apos;: 0.013312755,
 &apos;Loss/RPNLoss/objectness_loss&apos;: 0.008382628,
 &apos;Loss/regularization_loss&apos;: 0.0,
 &apos;Loss/total_loss&apos;: 0.09418628,
 &apos;learning_rate&apos;: 0.0042}

&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;....&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Since there is an existing checkpoint, the trainer has picked up on it from a previous run.&lt;/p&gt;

&lt;p&gt;To view the training progress you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensorboard&lt;/code&gt; to point to the training subdirectory as follows:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensorboard --logdir /tfod_example/lisa/experiments/training
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that it may take some time before the mAP metrics show up in the dashboard.&lt;/p&gt;

&lt;p&gt;In conclusion, the TFOD is a robust framework to learn for prototyping object detection projects. Due to its complexity, it will take a while to get used to its intricacies but the effort is worth it in my opinion.&lt;/p&gt;

&lt;p&gt;For more information, consult the official &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot;&gt;TensorFlow Object Detection API&lt;/a&gt; github project page and the official documentation on &lt;a href=&quot;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md&quot;&gt;TFOD setup using TF 2&lt;/a&gt;. Pre-trained models can be found on &lt;a href=&quot;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md&quot;&gt;TF2 Model Zoo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Happy Hacking !&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate>
        <link>https://www.cheeyeo.dev/machine-learning/deep-learning/computer-vision/tensorflow/2021/11/03/using-tensorflow-object-detection-api/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/machine-learning/deep-learning/computer-vision/tensorflow/2021/11/03/using-tensorflow-object-detection-api/</guid>
      </item>
    
      <item>
        <title>Guide to installing CUDA on Ubuntu 18.04 LTS</title>
        <description>
&lt;p&gt;This guide attempts to highlight a process of installing CUDA 11 on UBUNTU 18.04 LTS.&lt;/p&gt;

&lt;p&gt;This post is inspired by the following &lt;a href=&quot;https://www.pyimagesearch.com/2019/12/09/how-to-install-tensorflow-2-0-on-ubuntu/&quot; target=&quot;_blank&quot;&gt;blog post on pyimagesearch on installing Tensorflow 2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Its not by any means a comprehensive guide as hardware differs but it aims to hopefully get you up and running asap.&lt;/p&gt;

&lt;p&gt;The steps documented below applies for both new installs or to update an existing CUDA install.&lt;/p&gt;

&lt;h3 id=&quot;hardware-and-operating-system-tested&quot;&gt;Hardware and operating system tested&lt;/h3&gt;

&lt;p&gt;Ubuntu 18.04 LTS with GeForce GTX 1060&lt;/p&gt;

&lt;h3 id=&quot;pre-install-step&quot;&gt;Pre-install step&lt;/h3&gt;

&lt;p&gt;Update and install system dependencies:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;sudo apt update &amp;amp;&amp;amp; sudo apt upgrade

sudo apt install build-essential \
cmake \
unzip \
pkg-config \
gcc-7 \
g++-7 \
libopenblas-dev \
libatlas-base-dev \
liblapack-dev \
gfortran \
python3-dev \
python3-tk \
python-imaging-tk&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;update--install-nvidia-device-driver&quot;&gt;Update / Install nvidia device driver&lt;/h3&gt;

&lt;p&gt;Add PPA for nvidia device drivers and install &lt;strong&gt;nvidia-driver-470&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CUDA 11.3 only works for versions of device drivers &amp;gt;= 465.&lt;/p&gt;

&lt;p&gt;Note: for Ubuntu, the 465 driver is linked to the 470 driver so there is no dedicated 465 version&lt;/p&gt;

&lt;p&gt;Install device driver:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;sudo add-apt-repository ppa:graphics-drivers/ppa

sudo apt-get install nvidia-driver-470&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;After install/update, reboot the system.&lt;/p&gt;

&lt;p&gt;Check that the device driver is working by running nvidia-smi&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;nvidia-smi&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If it works, you should see the device driver version and the GPU hardware in the display.&lt;/p&gt;

&lt;p&gt;As an additional sanity check, you can also bring up &lt;strong&gt;NVIDIA Xserver settings&lt;/strong&gt; and check that it has picked up the right device driver version and GPU.&lt;/p&gt;

&lt;p&gt;Note that the nvidia-smi utility is installed through the drivers and is independent of CUDA.&lt;/p&gt;

&lt;h3 id=&quot;installing-cuda&quot;&gt;Installing CUDA&lt;/h3&gt;

&lt;p&gt;For the purposes of this guide we are installing CUDA 11.3 in order to install and run tensorflow 2.6.0. This is to overcome the issue of the missing &lt;strong&gt;libcudart.11.0&lt;/strong&gt; library.&lt;/p&gt;

&lt;p&gt;For CUDA 11.3, you need the device driver to be at least &amp;gt;= 465, hence we installed 470 of the driver above.&lt;/p&gt;

&lt;p&gt;Easiest way to install CUDA is to download and run the installer.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run

chmod +x cuda_11.3.0_465.19.01_linux.run

sudo ./cuda_11.3.0_465.19.01_linux.run --override&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will bring up an install screen. &lt;strong&gt;Uncheck&lt;/strong&gt; the 465 driver option. This is &lt;strong&gt;IMPORTANT&lt;/strong&gt; else it will corrupt the device driver since we have already installed it in step 1.&lt;/p&gt;

&lt;p&gt;Keep the remaining options as it is.&lt;/p&gt;

&lt;p&gt;After installation, it will copy the cuda libs to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/cuda-11.3&lt;/code&gt; and makes a symlink to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/cuda&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To check that cuda is installed, run &lt;strong&gt;nvcc&lt;/strong&gt; compiler:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;nvcc --version&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that the CUDA version reported in nvidia-smi will not match the current installed version.&lt;/p&gt;

&lt;p&gt;Update &lt;strong&gt;~/.bashrc&lt;/strong&gt; by setting the LD_PATH and PATH variables for cuda:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;gp&quot;&gt;export PATH=/usr/local/cuda/bin:$&lt;/span&gt;PATH
&lt;span class=&quot;go&quot;&gt;export LD_LIBRARY_PATH=/usr/local/cuda/lib64&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;install-cudnn&quot;&gt;Install CUDNN&lt;/h3&gt;

&lt;p&gt;The CUDNN library is required by tensorflow.&lt;/p&gt;

&lt;p&gt;The approach I took was to install using the deb file from the nvidia cuda repo. The version of libcudnn after the &lt;strong&gt;+&lt;/strong&gt; symbol has to match with the installed cuda version.&lt;/p&gt;

&lt;p&gt;For example, if we have cuda 11.3 then we need to install the deb files with &lt;strong&gt;..+cuda11.3..&lt;/strong&gt; in the suffix.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/libcudnn8_8.2.1.32-1+cuda11.3_amd64.deb

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/libcudnn8-dev_8.2.1.32-1+cuda11.3_amd64.deb

sudo dpkg -i libcudnn8_8.2.1.32-1+cuda11.3_amd64.deb

sudo dpkg -i libcudnn8-dev_8.2.1.32-1+cuda11.3_amd64.deb

sudo ldconfig&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You need to ensure that you don’t have existing versions of CUDNN before installing a newer version. TF will pick up the older version and will throw a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mismatch CUDNN version&lt;/code&gt; error during invocation.&lt;/p&gt;

&lt;h3 id=&quot;install-tensorrt-libs&quot;&gt;Install TensorRT libs&lt;/h3&gt;

&lt;p&gt;To run TensorRT, we need to install the libnvinfer libraries. These would require cuda-nvrtc libraries as dependencies else the install would fail.&lt;/p&gt;

&lt;p&gt;Again, ensure that the cuda versions in the filenames match the actual installed cuda version.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-nvrtc-11-3_11.3.58-1_amd64.deb

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-nvrtc-dev-11-3_11.3.58-1_amd64.deb

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/libnvinfer8_8.0.3-1+cuda11.3_amd64.deb

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/libnvinfer-dev_8_8.0.3-1+cuda11.3_amd64.deb

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/libnvinfer-plugin8_8.0.3-1+cuda11.3_amd64.deb

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/libnvinfer-plugin-dev_8_8.0.3-1+cuda11.3_amd64.deb

sudo dpkg -i cuda-nvrtc-11-3_11.3.58-1_amd64.deb \
cuda-nvrtc-dev-11-3_11.3.58-1_amd64.deb \
libnvinfer8_8.0.3-1+cuda11.3_amd64.deb \
libnvinfer-dev_8_8.0.3-1+cuda11.3_amd64.deb \
libnvinfer-plugin8_8.0.3-1+cuda11.3_amd64.deb \
libnvinfer-plugin-dev_8_8.0.3-1+cuda11.3_amd64.deb

sudo ldconfig&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;install-tensorflow&quot;&gt;Install Tensorflow&lt;/h3&gt;

&lt;p&gt;I tend to create a venv to test any new install of tensorflow as it has multiple dependencies which may or may not conflict with existing packages.&lt;/p&gt;

&lt;p&gt;Firstly, we need to export the LD_PATH to include CUPTI from CUDA. Then we create a venv and install tensorflow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CUPTI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib64&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;venv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;venv&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;venv&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activate&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;test-tensorflow-install&quot;&gt;Test tensorflow install&lt;/h3&gt;

&lt;p&gt;While still in activated venv, run following test script:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[INFO] Checking TF Gpu installed ...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_physical_devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;GPU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TF VERSION: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__version__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debugging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_log_device_placement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If all goes well, should see output similar to this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;....&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PhysicalDevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;/physical_device:GPU:0&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;GPU&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;....&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;45.114704&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common_runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1510&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Created&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localhost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replica&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5018&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MB&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NVIDIA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeForce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GTX&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1060&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Design&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pci&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bus&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;00.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capability&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;45.172025&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common_runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eager&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1161&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Executing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_EagerConst&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localhost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replica&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;45.172447&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common_runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eager&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1161&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Executing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_EagerConst&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localhost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replica&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;45.173005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common_runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eager&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1161&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Executing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatMul&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localhost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replica&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;22.&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;28.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;49.&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;64.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Important lines are &lt;strong&gt;device /job:localhost/replica:0/task:0/device:GPU:0&lt;/strong&gt; which indicates that TF is able to locate and access the GPU device.&lt;/p&gt;

&lt;h3 id=&quot;pre-built-docker-images&quot;&gt;Pre-built docker images&lt;/h3&gt;

&lt;p&gt;An alternative is to run tensorflow locally using one of the &lt;a href=&quot;https://hub.docker.com/r/tensorflow/tensorflow/tags&quot; target=&quot;_blank&quot;&gt;prebuilt Tensorflow docker image&lt;/a&gt; and bind-mount a local directory into the running container:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-console&quot; data-lang=&quot;console&quot;&gt;&lt;span class=&quot;go&quot;&gt;docker pull tensorflow/tensorflow:2.6.0

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;docker run --gpus all -it --rm -v &amp;lt;source path&amp;gt;&lt;/span&gt;:&amp;lt;target container path&amp;gt; &lt;span class=&quot;nt&quot;&gt;--entrypoint&lt;/span&gt; /bin/bash tensorflow/tensorflow-gpu:2.6.0
&lt;span class=&quot;go&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;cd &amp;lt;target container path&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
</description>
        <pubDate>Wed, 27 Oct 2021 01:00:00 +0100</pubDate>
        <link>https://www.cheeyeo.dev/cuda/machine-learning/mlops/2021/10/27/guide-to-installing-cuda/</link>
        <guid isPermaLink="true">https://www.cheeyeo.dev/cuda/machine-learning/mlops/2021/10/27/guide-to-installing-cuda/</guid>
      </item>
    
  </channel>
</rss>
