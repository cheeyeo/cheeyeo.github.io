<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Using Flink High Availability with flink operator &#8211; Blog of software writer Chee Yeo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="How to enable and use flink high availability">
    <meta name="author" content="Chee Yeo">
    <meta name="keywords" content="datascience, beam, flink, pyflink, kubernetes, kind">
    <link rel="canonical" href="https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/2022/12/02/flink-high-availability/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Blog of software writer Chee Yeo" href="/feed.xml" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202301282159" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_UK">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Using Flink High Availability with flink operator">
    <meta property="og:description" content="Chee Yeo is a software developer with interests in machine learning and cloud computing.">
    <meta property="og:url" content="https://www.cheeyeo.dev/datascience/beam/flink/pyflink/kubernetes/kind/2022/12/02/flink-high-availability/">
    <meta property="og:site_name" content="Blog of software writer Chee Yeo">
</head>

<body class="">
  <div class="color-line"></div>
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      
        <nav class="site-nav">
          <a href="/">Home</a>
          <a href="https://tilrnt.github.io/" target="_blank">TILRNT</a>
          <a href="/about">About</a>
          <a href="/contact">Contact</a>
        </nav>
      
      <div class="clearfix"></div>
    </div>
  </div>
</header>

    <header class="blog-header">
  <h1 class="blog-title">Using Flink High Availability with flink operator</h1>

  
  <div class="meta_info">
    
    <div class="author-date-wrap">
      <div class="author">
        <a href="/about">Chee Yeo</a>
      </div>
    </div>
    
    <span class="post-date">December 2, 2022</span>
    
    <ul class="article-tag">
      
      <li>
        <a href="/categories/datascience">datascience</a>
      </li>
      
      <li>
        <a href="/categories/beam">beam</a>
      </li>
      
      <li>
        <a href="/categories/flink">flink</a>
      </li>
      
      <li>
        <a href="/categories/pyflink">pyflink</a>
      </li>
      
      <li>
        <a href="/categories/kubernetes">kubernetes</a>
      </li>
      
      <li>
        <a href="/categories/kind">kind</a>
      </li>
      
    </ul>
    
  </div>
  
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<article class="post-content top-border">
  
<p>Flink supports high-availability mode for both standalone and native Kubernetes via the Flink operator. This article aims to explain the purpose of HA and how to configure and run it in a local kind cluster.</p>

<p>A flink cluster has only 1 JobManager running at a given point in time. This presents as single point of failure. If the jobmanager fails currently running jobs within the cluster will also fail and have to be restarted from scratch.</p>

<p>Enabling HA allows the cluster to recover from such failures and ensures that streaming jobs especially can resume from its last known state via checkpoints.</p>

<p>In a previous blog post, I mentioned <code class="language-plaintext highlighter-rouge">savepoints</code> and how we can resume a job from it. <code class="language-plaintext highlighter-rouge">checkpoints</code> is a mechanism provided via the HA service. When HA is enabled, for any streaming job, Flink will make regular backups of the jobâ€™s state via <code class="language-plaintext highlighter-rouge">checkpoints</code> which allows you to resume the job from in event of a cluster failure.</p>

<p>The main difference between <code class="language-plaintext highlighter-rouge">savepoints</code> and <code class="language-plaintext highlighter-rouge">checkpoints</code> is that the former is triggred by the user while the other is managed entirely by Flink.</p>

<p>The main purpose of having two complementary systems is that <code class="language-plaintext highlighter-rouge">checkpoints</code> provide fast recoverable state in the event of cluster failures such as job manager or task manager pods being killed whereas <code class="language-plaintext highlighter-rouge">savepoints</code> allow for more portability and is intended for long-term uses such as Flink versions upgrade and changes to job properties.</p>

<p>In HA mode, we can have more than 1 job manager pod running concurrently and only 1 of them is selected as the Leader via the leader election service.</p>

<p>In this post, we are using the <code class="language-plaintext highlighter-rouge">flink-operator</code> to setup our session cluster. The default setting creates a stateful set of the jobmanager and the number of replicas is not configurable at this point. However, running a replicaset means there will always be at least 1 job manager pod running so it serves this use case.</p>

<p>To enable HA, the following conditions must be met:</p>

<ul>
  <li>
    <p>Only use local storage for the high availability checkpoints. In the kind cluster config, we can mount an additional local volume and reference it in a persistent volume. The mounted volume must also have the ownership of <code class="language-plaintext highlighter-rouge">9999:9999</code></p>
  </li>
  <li>
    <p>A service account which has permissions to edit configmaps. HA stores information on the cluster state such as the current jobmanager in these configmaps.</p>
  </li>
</ul>

<p>To create the HA volume I mounted a local volume in /tmp/flink-k8s-example on localhost to /flink-k8s-example on the node in the kind cluster config:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kind.x-k8s.io/v1alpha4</span>
<span class="na">nodes</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">role</span><span class="pi">:</span> <span class="s">control-plane</span>
  <span class="na">kubeadmConfigPatches</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="pi">|</span>
    <span class="s">kind: InitConfiguration</span>
    <span class="s">nodeRegistration:</span>
      <span class="s">kubeletExtraArgs:</span>
        <span class="s">node-labels: "ingress-ready=true"</span>
  <span class="na">extraPortMappings</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">hostPort</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">hostPort</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">extraMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">hostPath</span><span class="pi">:</span> <span class="s">/tmp/artifacts</span>
      <span class="na">containerPath</span><span class="pi">:</span> <span class="s">/artifacts</span>
    <span class="pi">-</span> <span class="na">hostPath</span><span class="pi">:</span> <span class="s">/tmp/flink-k8s-example</span>
      <span class="na">containerPath</span><span class="pi">:</span> <span class="s">/flink-k8s-example</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Next we create the PV, and PVC for the mounted volume:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">flink-pv</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">local</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">manual</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/flink-k8s-example/</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">flink-shared-pvc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">manual</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">volumeName</span><span class="pi">:</span> <span class="s">flink-pv</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The PV mounts the <code class="language-plaintext highlighter-rouge">/flink-k8s-example</code> path on the node to create a volume.</p>

<p>The service account can be created from the default cluster service account:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>kubectl create serviceaccount flink-service-account

kubectl create clusterrolebinding flink-role-binding-flink <span class="nt">--clusterrole</span><span class="o">=</span>edit <span class="nt">--serviceaccount</span><span class="o">=</span>default:flink-service-account
</pre></td></tr></tbody></table></code></pre></figure>

<p>We reference the service account in the flink cluster config:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">flinkoperator.k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">FlinkCluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">beam-flink-cluster</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">flinkVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1.15.2"</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">apache/flink:1.15.2</span>
  <span class="na">serviceAccountName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">flink-service-account"</span>
  
  <span class="s">...</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The flink config needs to be updated to include the HA config:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="code"><pre><span class="na">taskmanager.memory.process.size</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2g"</span>
<span class="na">taskmanager.data.port</span><span class="pi">:</span> <span class="s2">"</span><span class="s">6121"</span>
<span class="na">taskmanager.numberOfTaskSlots</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3"</span>
<span class="na">parallelism.default</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>

<span class="na">state.backend</span><span class="pi">:</span> <span class="s">filesystem</span>
<span class="na">state.backend.incremental</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
<span class="na">state.checkpoints.dir</span><span class="pi">:</span> <span class="s">file:///flink-shared/checkpoints</span>
<span class="na">state.savepoints.dir</span><span class="pi">:</span> <span class="s">file:///flink-shared/savepoints</span>

<span class="na">classloader.resolve-order</span><span class="pi">:</span> <span class="s">parent-first</span>

<span class="na">execution.checkpointing.interval</span><span class="pi">:</span> <span class="s2">"</span><span class="s">60"</span>

<span class="c1"># Kubernetes config</span>
<span class="na">kubernetes.cluster-id</span><span class="pi">:</span> <span class="s2">"</span><span class="s">beam-flink-cluster"</span>
<span class="na">kubernetes.taskmanager.service-account</span><span class="pi">:</span> <span class="s2">"</span><span class="s">flink-service-account"</span>

<span class="c1"># Below for HA config</span>
<span class="na">high-availability</span><span class="pi">:</span> <span class="s">org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory</span>
<span class="na">high-availability.jobmanager.port</span><span class="pi">:</span> <span class="s2">"</span><span class="s">50010"</span>
<span class="na">high-availability.storageDir</span><span class="pi">:</span> <span class="s">file:///flink-shared/ha</span>
<span class="na">high-availability.cluster-id</span><span class="pi">:</span> <span class="s2">"</span><span class="s">beam-flink-cluster"</span>
<span class="na">restart-strategy</span><span class="pi">:</span> <span class="s">fixed-delay</span>
<span class="na">restart-strategy.fixed-delay.attempts</span><span class="pi">:</span> <span class="s2">"</span><span class="s">10"</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>We are using the built in HA services factory class for high-availability. We mount the volume created previously as state storage and we set the cluster id to be the cluster name, which is required.</p>

<p>Next we define the restart strategy and a timeout. As a precaution, I also pined the jobmanager port to 50010 as the configuration docs states that this port can be a random value when a new jobmanager is created.</p>

<p>We define the state checkpoint interval to be 60. Note that this value must be greater than 0 for checkpointing to work. We define the cluster id, which is set to the name of the flink cluster. We also add the custom service account to the taskmanager.</p>

<p>The <code class="language-plaintext highlighter-rouge">state.backend</code> is set to filesystem. We also enable incremental checkpoint via <code class="language-plaintext highlighter-rouge">state.backend.incremental</code> which only stores diffs of checkpoints rather than entire checkpoints. Note that the <code class="language-plaintext highlighter-rouge">state.checkpoints.dir</code> and <code class="language-plaintext highlighter-rouge">state.savepoints.dir</code> can also be set to remote storage locations such as s3, but the main <code class="language-plaintext highlighter-rouge">high-availability.storageDir</code> has to be set to a volume.</p>

<p>Assuming the setup is right, when we start the flink session cluster, we should see the following in the jobmanager logs:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre></td><td class="code"><pre>2022-12-11 15:29:32,308 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector <span class="o">[]</span> - Create KubernetesLeaderElector beam-flink-cluster-cluster-config-map with lock identity 3e81a3e8-b718-4c9e-96ad-cd8f0eacd48e.

2022-12-11 15:29:32,309 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer <span class="o">[]</span> - Starting to watch <span class="k">for </span>default/beam-flink-cluster-cluster-config-map, watching <span class="nb">id</span>:db45acf1-52b3-4239-91a6-7232c1c56bba
...

2022-12-11 15:29:32,378 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector <span class="o">[]</span> - New leader elected 3e81a3e8-b718-4c9e-96ad-cd8f0eacd48e <span class="k">for </span>beam-flink-cluster-cluster-config-map.

...

2022-12-11 15:29:32,541 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService <span class="o">[]</span> - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@8b670c0.

2022-12-11 15:29:32,541 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   <span class="o">[]</span> - Web frontend listening at http://beam-flink-cluster-jobmanager:8081.

2022-12-11 15:29:32,541 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   <span class="o">[]</span> - http://beam-flink-cluster-jobmanager:8081 was granted leadership with <span class="nv">leaderSessionID</span><span class="o">=</span>92a23e47-73ea-4564-890f-0cb39937a15a

...

2022-12-11 15:29:32,556 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService <span class="o">[]</span> - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@3513d214.

2022-12-11 15:29:32,556 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl <span class="o">[]</span> - Starting resource manager service.

2022-12-11 15:29:32,556 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService <span class="o">[]</span> - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@7534785a.

2022-12-11 15:29:32,557 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService <span class="o">[]</span> - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span><span class="nb">.</span>

2022-12-11 15:29:32,557 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService <span class="o">[]</span> - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span><span class="nb">.</span>

2022-12-11 15:29:32,572 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer <span class="o">[]</span> - Starting to watch <span class="k">for </span>default/beam-flink-cluster-cluster-config-map, watching <span class="nb">id</span>:c389c018-0bda-437a-aea3-656aa64dc47f

2022-12-11 15:29:32,572 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer <span class="o">[]</span> - Starting to watch <span class="k">for </span>default/beam-flink-cluster-cluster-config-map, watching <span class="nb">id</span>:d25f089c-67eb-4211-8ca2-245e55409c37

2022-12-11 15:29:32,574 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner <span class="o">[]</span> - DefaultDispatcherRunner was granted leadership with leader <span class="nb">id </span>92a23e47-73ea-4564-890f-0cb39937a15a. Creating new DispatcherLeaderProcess.

2022-12-11 15:29:32,581 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Start SessionDispatcherLeaderProcess.

2022-12-11 15:29:32,584 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl <span class="o">[]</span> - Resource manager service is granted leadership with session <span class="nb">id </span>92a23e47-73ea-4564-890f-0cb39937a15a.

2022-12-11 15:29:32,585 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Recover all persisted job graphs that are not finished, yet.

2022-12-11 15:29:32,607 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             <span class="o">[]</span> - Starting RPC endpoint <span class="k">for </span>org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_0 <span class="nb">.</span>

2022-12-11 15:29:32,619 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     <span class="o">[]</span> - Retrieved job ids <span class="o">[]</span> from KubernetesStateHandleStore<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span>
2022-12-11 15:29:32,619 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Successfully recovered 0 persisted job graphs.
...
</pre></td></tr></tbody></table></code></pre></figure>

<p>The logs show that the leader election service is activated and the current job manager http://beam-flink-cluster-jobmanager:8081 is selected to be the leader. Note that the service is created automatically via the flink-operator in this case. It then creates a dispatcher and resource manager service and assign them as leader, updating the configmaps.</p>

<p>HA automatically tracks the current leader via configmaps. It created two configmaps: <code class="language-plaintext highlighter-rouge">&lt;flink cluster name&gt;-cluster-config-map</code> and <code class="language-plaintext highlighter-rouge">&lt;flink cluster name&gt;-configmap</code>. The first configmap contains the leader election details while the second configmap contains a copy of the flink and log4j configs used in the initial cluster setup.</p>

<p>We can use the following failure scenarios to test if HA is actually working:</p>

<h4 id="kill-the-current-jobmanager-pod-process">Kill the current jobmanager pod process</h4>

<p>This will terminate the jobmanager process. The logs should show it being restarted:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="o">{</span>jobmanager_pod_name<span class="o">}</span> <span class="nt">--</span> /bin/sh <span class="nt">-c</span> <span class="s2">"kill 1"</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Output of kubectl get pods:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>beam-flink-cluster-jobmanager-0    1/1     Running   1 <span class="o">(</span>55s ago<span class="o">)</span>   19m
beam-flink-cluster-taskmanager-0   2/2     Running   0             19m
beam-flink-cluster-taskmanager-1   2/2     Running   0             19m
</pre></td></tr></tbody></table></code></pre></figure>

<p>The jobmanager logs show the same startup information as when the cluster was first created, suggesting that a new jobmanager pod was created.</p>

<h4 id="kill-a-taskmanager-pod-process">Kill a taskmanager pod process</h4>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="o">{</span>taskmanager_pod_name<span class="o">}</span> <span class="nt">--</span> /bin/sh <span class="nt">-c</span> <span class="s2">"kill 1"</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Output of the jobmanager logs shows that a new taskmanager process is started and registered:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>2022-12-11 15:51:47,574 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager <span class="o">[]</span> - Closing TaskExecutor connection 10.244.0.18:6122-3821cd because: The TaskExecutor is shutting down.

2022-12-11 15:52:01,144 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager <span class="o">[]</span> - Registering TaskManager with ResourceID 10.244.0.18:6122-fe0c19 <span class="o">(</span>akka.tcp://flink@10.244.0.18:6122/user/rpc/taskmanager_0<span class="o">)</span> at ResourceManager
...
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="delete-the-jobmanager-pod">Delete the jobmanager pod</h4>

<p>Delete the main jobmanager pod using:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>kubectl delete pod <span class="o">{</span>jobmanager_pod_name<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Output of jobmanager logs:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre></td><td class="code"><pre>2022-12-11 15:55:28,061 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        <span class="o">[]</span> - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.

2022-12-11 15:55:28,064 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        <span class="o">[]</span> - Shutting StandaloneSessionClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..

2022-12-11 15:55:28,064 INFO  org.apache.flink.runtime.blob.BlobServer                     <span class="o">[]</span> - Stopped BLOB server at 0.0.0.0:6124

2022-12-11 15:55:28,068 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   <span class="o">[]</span> - Shutting down rest endpoint.

2022-12-11 15:55:28,078 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   <span class="o">[]</span> - Removing cache directory /tmp/flink-web-0b1e5be0-95c2-4716-9688-44ef1748278b/flink-web-ui

2022-12-11 15:55:28,080 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService <span class="o">[]</span> - Stopping DefaultLeaderElectionService.

2022-12-11 15:55:28,090 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   <span class="o">[]</span> - Shut down complete.

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent <span class="o">[]</span> - Closing components.

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService <span class="o">[]</span> - Stopping DefaultLeaderRetrievalService.

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver <span class="o">[]</span> - Stopping KubernetesLeaderRetrievalDriver<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span><span class="nb">.</span>

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService <span class="o">[]</span> - Stopping DefaultLeaderRetrievalService.

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver <span class="o">[]</span> - Stopping KubernetesLeaderRetrievalDriver<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span><span class="nb">.</span>

2022-12-11 15:55:28,091 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService <span class="o">[]</span> - Stopping DefaultLeaderElectionService.

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer <span class="o">[]</span> - Stopped to watch <span class="k">for </span>default/beam-flink-cluster-cluster-config-map, watching <span class="nb">id</span>:dbf98f2b-6765-45be-bf62-afc1f15d84f6

2022-12-11 15:55:28,091 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer <span class="o">[]</span> - Stopped to watch <span class="k">for </span>default/beam-flink-cluster-cluster-config-map, watching <span class="nb">id</span>:a2cc3271-7ef2-47ee-9342-dfcbc83bbc4a

2022-12-11 15:55:28,110 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Stopping SessionDispatcherLeaderProcess.

2022-12-11 15:55:28,110 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl <span class="o">[]</span> - Stopping resource manager service.

2022-12-11 15:55:28,110 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     <span class="o">[]</span> - Stopping dispatcher akka.tcp://flink@beam-flink-cluster-jobmanager:6123/user/rpc/dispatcher_1.

2022-12-11 15:55:28,111 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService <span class="o">[]</span> - 
Stopping DefaultLeaderElectionService.

2022-12-11 15:55:28,111 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     <span class="o">[]</span> - Stopping all currently running <span class="nb">jobs </span>of dispatcher akka.tcp://flink@beam-flink-cluster-jobmanager:6123/user/rpc/dispatcher_1.

2022-12-11 15:55:28,112 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     <span class="o">[]</span> - Stopped dispatcher akka.tcp://flink@beam-flink-cluster-jobmanager:6123/user/rpc/dispatcher_1.

2022-12-11 15:55:28,114 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     <span class="o">[]</span> - Stopping DefaultJobGraphStore.

2022-12-11 15:55:28,117 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager <span class="o">[]</span> - Closing the slot manager.

2022-12-11 15:55:28,117 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager <span class="o">[]</span> - Suspending the slot manager.

2022-12-11 15:55:28,119 INFO  org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService <span class="o">[]</span> - Closing DefaultMultipleComponentLeaderElectionService.

2022-12-11 15:55:28,120 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver <span class="o">[]</span> - Closing org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver@5bb0c5c0.
</pre></td></tr></tbody></table></code></pre></figure>

<p>The logs indicate that the leader election service was indeed closed when the jobmanager pod is deleted.</p>

<p>The output of the new jobmanager pod show that the new jobmanager pod is elected as the current leader:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>2022-12-11 15:55:36,536 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector <span class="o">[]</span> - Create KubernetesLeaderElector beam-flink-cluster-cluster-config-map with lock identity 05df88b5-bbd2-400f-b1e8-9d4d386b2a43.

2022-12-11 15:55:36,537 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer <span class="o">[]</span> - Starting to watch <span class="k">for </span>default/beam-flink-cluster-cluster-config-map, watching <span class="nb">id</span>:fa91664e-7df9-4937-802b-20e3cba09bc9

...

2022-12-11 15:55:46,149 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   <span class="o">[]</span> - http://beam-flink-cluster-jobmanager:8081 was granted leadership with <span class="nv">leaderSessionID</span><span class="o">=</span>28855791-dee8-4a20-9cd1-b1d5b91be838

2022-12-11 15:55:46,149 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl <span class="o">[]</span> - Resource manager service is granted leadership with session <span class="nb">id </span>28855791-dee8-4a20-9cd1-b1d5b91be838.
...
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="testing-ha-with-checkpoints">Testing HA with checkpoints</h3>

<p>We can use the built-in statemachine example to simulate a long running streaming job. Then we monitor the checkpoints directory to ensure that its created.</p>

<p><img src="/assets/img/flink/running-streamjob.png" alt="Running statemachine streaming job" /></p>

<p>The jobmanager logs should show the job running and checkpoints being saved:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>2022-12-11 16:02:21,632 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       <span class="o">[]</span> - Flat Map -&gt; Sink: Print to Std. Out <span class="o">(</span>1/1<span class="o">)</span> <span class="o">(</span>e4adc6c6d1d4b2b9e9318cfd95e65e35<span class="o">)</span> switched from INITIALIZING to RUNNING.

2022-12-11 16:02:23,370 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Triggering checkpoint 1 <span class="o">(</span><span class="nb">type</span><span class="o">=</span>CheckpointType<span class="o">{</span><span class="nv">name</span><span class="o">=</span><span class="s1">'Checkpoint'</span>, <span class="nv">sharingFilesStrategy</span><span class="o">=</span>FORWARD_BACKWARD<span class="o">})</span> @ 1670774543316 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:02:23,451 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Completed checkpoint 1 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4 <span class="o">(</span>7735 bytes, <span class="nv">checkpointDuration</span><span class="o">=</span>123 ms, <span class="nv">finalizationTime</span><span class="o">=</span>11 ms<span class="o">)</span><span class="nb">.</span>

2022-12-11 16:02:25,326 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Triggering checkpoint 2 <span class="o">(</span><span class="nb">type</span><span class="o">=</span>CheckpointType<span class="o">{</span><span class="nv">name</span><span class="o">=</span><span class="s1">'Checkpoint'</span>, <span class="nv">sharingFilesStrategy</span><span class="o">=</span>FORWARD_BACKWARD<span class="o">})</span> @ 1670774545316 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:02:25,364 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Completed checkpoint 2 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4 <span class="o">(</span>8320 bytes, <span class="nv">checkpointDuration</span><span class="o">=</span>23 ms, <span class="nv">finalizationTime</span><span class="o">=</span>25 ms<span class="o">)</span><span class="nb">.</span>

...
</pre></td></tr></tbody></table></code></pre></figure>

<p>We can try to kill the jobmanager process and it should resume the job from the last checkpoint, which was checkpoint 106 for this example:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="code"><pre>2022-12-11 16:06:08,109 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Recover all persisted job graphs that are not finished, yet.

2022-12-11 16:06:08,187 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     <span class="o">[]</span> - Retrieved job ids <span class="o">[</span>c506b6c290cc2d96a0e3f0eea10395c4] from KubernetesStateHandleStore<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span>

2022-12-11 16:06:08,188 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Trying to recover job with job <span class="nb">id </span>c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:06:08,406 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     <span class="o">[]</span> - Recovered JobGraph<span class="o">(</span>jobId: c506b6c290cc2d96a0e3f0eea10395c4<span class="o">)</span><span class="nb">.</span>

2022-12-11 16:06:08,407 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess <span class="o">[]</span> - Successfully recovered 1 persisted job graphs.

2022-12-11 16:06:09,128 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 <span class="o">[]</span> - Initializing job <span class="s1">'State machine job'</span> <span class="o">(</span>c506b6c290cc2d96a0e3f0eea10395c4<span class="o">)</span><span class="nb">.</span>

2022-12-11 16:06:09,209 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 <span class="o">[]</span> - Using restart back off <span class="nb">time </span>strategy FixedDelayRestartBackoffTimeStrategy<span class="o">(</span><span class="nv">maxNumberRestartAttempts</span><span class="o">=</span>10, <span class="nv">backoffTimeMS</span><span class="o">=</span>1000<span class="o">)</span> <span class="k">for </span>State machine job <span class="o">(</span>c506b6c290cc2d96a0e3f0eea10395c4<span class="o">)</span><span class="nb">.</span>

2022-12-11 16:06:09,230 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils <span class="o">[]</span> - Recovering checkpoints from KubernetesStateHandleStore<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-c506b6c290cc2d96a0e3f0eea10395c4-config-map'</span><span class="o">}</span><span class="nb">.</span>

2022-12-11 16:06:09,276 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils <span class="o">[]</span> - Found 1 checkpoints <span class="k">in </span>KubernetesStateHandleStore<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-c506b6c290cc2d96a0e3f0eea10395c4-config-map'</span><span class="o">}</span><span class="nb">.</span>

2022-12-11 16:06:09,277 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils <span class="o">[]</span> - Trying to fetch 1 checkpoints from storage.

2022-12-11 16:06:09,277 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils <span class="o">[]</span> - Trying to retrieve checkpoint 106.
...

2022-12-11 16:06:09,504 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 <span class="o">[]</span> - Starting execution of job <span class="s1">'State machine job'</span> <span class="o">(</span>c506b6c290cc2d96a0e3f0eea10395c4<span class="o">)</span> under job master <span class="nb">id </span>8c15f93c28f74b20cc1b30ccb4da4cc3.

2022-12-11 16:06:09,506 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 <span class="o">[]</span> - Starting scheduling with scheduling strategy <span class="o">[</span>org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]

2022-12-11 16:06:09,507 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       <span class="o">[]</span> - Job State machine job <span class="o">(</span>c506b6c290cc2d96a0e3f0eea10395c4<span class="o">)</span> switched from state CREATED to RUNNING.
...

2022-12-11 16:06:18,799 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Triggering checkpoint 107 <span class="o">(</span><span class="nb">type</span><span class="o">=</span>CheckpointType<span class="o">{</span><span class="nv">name</span><span class="o">=</span><span class="s1">'Checkpoint'</span>, <span class="nv">sharingFilesStrategy</span><span class="o">=</span>FORWARD_BACKWARD<span class="o">})</span> @ 1670774778787 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:06:18,884 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Completed checkpoint 107 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4 <span class="o">(</span>8338 bytes, <span class="nv">checkpointDuration</span><span class="o">=</span>72 ms, <span class="nv">finalizationTime</span><span class="o">=</span>25 ms<span class="o">)</span><span class="nb">.</span>

2022-12-11 16:06:20,803 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Triggering checkpoint 108 <span class="o">(</span><span class="nb">type</span><span class="o">=</span>CheckpointType<span class="o">{</span><span class="nv">name</span><span class="o">=</span><span class="s1">'Checkpoint'</span>, <span class="nv">sharingFilesStrategy</span><span class="o">=</span>FORWARD_BACKWARD<span class="o">})</span> @ 1670774780787 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4.

2022-12-11 16:06:20,843 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    <span class="o">[]</span> - Completed checkpoint 108 <span class="k">for </span>job c506b6c290cc2d96a0e3f0eea10395c4 <span class="o">(</span>15169 bytes, <span class="nv">checkpointDuration</span><span class="o">=</span>32 ms, <span class="nv">finalizationTime</span><span class="o">=</span>24 ms<span class="o">)</span><span class="nb">.</span>
...
</pre></td></tr></tbody></table></code></pre></figure>

<p>As can be seen above, the job was restored and continued to create checkpoints from its last checkpoint.</p>

<p>When the job is stopped/cancelled manually, the HA data, including the checkpoints will also be automatically deleted:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>...

2022-12-11 16:16:34,782 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices <span class="o">[]</span> - Clean up the high availability data <span class="k">for </span>job 034b3fe2d4f673aed68b38e787f8edf0.

2022-12-11 16:16:34,788 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices <span class="o">[]</span> - Finished cleaning up the high availability data <span class="k">for </span>job 034b3fe2d4f673aed68b38e787f8edf0.

2022-12-11 16:16:34,797 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     <span class="o">[]</span> - Removed job graph 034b3fe2d4f673aed68b38e787f8edf0 from KubernetesStateHandleStore<span class="o">{</span><span class="nv">configMapName</span><span class="o">=</span><span class="s1">'beam-flink-cluster-cluster-config-map'</span><span class="o">}</span>.
</pre></td></tr></tbody></table></code></pre></figure>

<p>This is because checkpoints, by default, are only used to resume a job from failures. However you can set the application configuration to retain the checkpoint on job cancellation.</p>

<p>Further information can be found in the <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-144:+Native+Kubernetes+HA+for+Flink#FLIP144:NativeKubernetesHAforFlink-ConfigMap">Documentation on Flink HA</a>. Details about Flink configuration can be found on <a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/config/">Flink 1.15.2 configuration</a> page. More information can also be found on <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints/">Flink Checkpoints</a> and <a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/">Checkpoints vs savepoints</a>.</p>

<h3 id="summary">Summary</h3>

<p>This post attempts to explain how to setup high availability for a flink cluster running locally in a kind cluster but the same can be applied in an actual deployed flink cluster. In that case, the state backend should be changed to <code class="language-plaintext highlighter-rouge">rocksdb</code> as well as other tweaks to the configuration which is for another article.</p>

<p>H4ppy H4ck1n6 !!!</p>

</article>





      </div>
    </div>
  </div>

  <footer class="footer">
  <div class="p2 wrap">
    
      <div class="social-icons">
  <div class="">
    
      <a class="fa fa-github fa-lg" href="https://github.com/cheeyeo" target="_blank"></a>
    
    <a class="fa fa-rss fa-lg" href="https://www.cheeyeo.dev/feed.xml" target="_blank"></a>
    
    
    
      <a class="fa fa-envelope fa-lg" href="mailto:f/mnqwaypk"></a>
    
    
      <a class="fa fa-linkedin fa-lg" href="https://www.linkedin.com/in/cheeyeo" target="_blank"></a>
    
  </div>
</div>

    

    <div class="measure mt1 center">
      <strong>Â© 2022 Chee Yeo</strong><br/>
      <small>
        Built using the Pixll theme available on <a href="https://github.com/johno/pixyll" target="_blank">Github</a>.
      </small>
    </div>
  </div>
</footer>



</body>
</html>
